{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from pydrake.all import (\n",
    "    DiagramBuilder,\n",
    "    Simulator,\n",
    "    StartMeshcat,\n",
    "    RigidTransform,\n",
    "    RotationMatrix,\n",
    "    Sphere,\n",
    "    Box,\n",
    "    RollPitchYaw,\n",
    "    InverseKinematics,\n",
    "    Solve,\n",
    ")\n",
    "from pydrake.geometry import Rgba\n",
    "from pydrake.systems.sensors import RgbdSensor, CameraConfig\n",
    "from manipulation.station import LoadScenario, MakeHardwareStation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:drake:Meshcat listening for connections at http://localhost:7002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meshcat: http://localhost:7002\n"
     ]
    }
   ],
   "source": [
    "meshcat = StartMeshcat()\n",
    "print(f\"Meshcat: {meshcat.web_url()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perception functions\n",
    "\n",
    "def depth_image_to_point_cloud(depth_image, rgb_image, camera_info):\n",
    "    \"\"\"Convert depth image to colored point cloud in camera frame\"\"\"\n",
    "    height, width = depth_image.shape\n",
    "    fx = camera_info.focal_x()\n",
    "    fy = camera_info.focal_y()\n",
    "    cx = camera_info.center_x()\n",
    "    cy = camera_info.center_y()\n",
    "    \n",
    "    # Pixel grid\n",
    "    u = np.arange(width)\n",
    "    v = np.arange(height)\n",
    "    u_grid, v_grid = np.meshgrid(u, v)\n",
    "    \n",
    "    # Valid depth mask\n",
    "    valid = (depth_image > 0.01) & (depth_image < 5.0)\n",
    "    \n",
    "    u_valid = u_grid[valid]\n",
    "    v_valid = v_grid[valid]\n",
    "    z_valid = depth_image[valid]\n",
    "    \n",
    "    # Back-project to 3D (camera frame: X right, Y down, Z forward)\n",
    "    x = (u_valid - cx) * z_valid / fx\n",
    "    y = (v_valid - cy) * z_valid / fy\n",
    "    z = z_valid\n",
    "    \n",
    "    points = np.column_stack([x, y, z])\n",
    "    colors = rgb_image[valid].astype(float) / 255.0\n",
    "    \n",
    "    return points, colors\n",
    "\n",
    "\n",
    "def transform_points_to_world(points, X_CameraToWorld):\n",
    "    \"\"\"Transform points from camera frame to world frame\"\"\"\n",
    "    points_world = []\n",
    "    for pt in points:\n",
    "        pt_world = X_CameraToWorld @ pt\n",
    "        points_world.append(pt_world)\n",
    "    return np.array(points_world)\n",
    "\n",
    "\n",
    "def segment_red_color(points, colors, red_min=0.5, other_max=0.35):\n",
    "    \"\"\"Segment red points using color thresholds\"\"\"\n",
    "    is_red = (\n",
    "        (colors[:, 0] > red_min) &      # R > 0.5\n",
    "        (colors[:, 1] < other_max) &    # G < 0.35\n",
    "        (colors[:, 2] < other_max)      # B < 0.35\n",
    "    )\n",
    "    return points[is_red]\n",
    "\n",
    "\n",
    "def compute_ball_center(points):\n",
    "    \"\"\"Compute centroid of point cluster\"\"\"\n",
    "    if len(points) == 0:\n",
    "        return None\n",
    "    return np.mean(points, axis=0)\n",
    "\n",
    "\n",
    "def simulate_rgbd_capture(camera_info, X_WorldToCamera, ball_positions, ball_colors, ball_radius=0.05):\n",
    "    \"\"\"Simulate RGB-D camera seeing balls (for testing without real scene graph objects)\"\"\"\n",
    "    width = camera_info.width()\n",
    "    height = camera_info.height()\n",
    "    fx = camera_info.focal_x()\n",
    "    fy = camera_info.focal_y()\n",
    "    cx = camera_info.center_x()\n",
    "    cy = camera_info.center_y()\n",
    "    \n",
    "    # Initialize images\n",
    "    rgb_image = np.ones((height, width, 3), dtype=np.uint8) * 50  # Dark background\n",
    "    depth_image = np.zeros((height, width), dtype=np.float32)\n",
    "    \n",
    "    # Transform balls to camera frame\n",
    "    X_CameraToWorld = X_WorldToCamera.inverse()\n",
    "    \n",
    "    for ball_pos, ball_color in zip(ball_positions, ball_colors):\n",
    "        ball_in_camera = X_CameraToWorld @ ball_pos\n",
    "        \n",
    "        if ball_in_camera[2] <= 0:\n",
    "            continue\n",
    "        \n",
    "        u_center = fx * ball_in_camera[0] / ball_in_camera[2] + cx\n",
    "        v_center = fy * ball_in_camera[1] / ball_in_camera[2] + cy\n",
    "        radius_px = int(fx * ball_radius / ball_in_camera[2])\n",
    "        \n",
    "        for v in range(max(0, int(v_center - radius_px)), min(height, int(v_center + radius_px))):\n",
    "            for u in range(max(0, int(u_center - radius_px)), min(width, int(u_center + radius_px))):\n",
    "                du = u - u_center\n",
    "                dv = v - v_center\n",
    "                \n",
    "                if du**2 + dv**2 < radius_px**2:\n",
    "                    offset_from_center = np.sqrt(du**2 + dv**2) / radius_px\n",
    "                    if offset_from_center < 1.0:\n",
    "                        depth_offset = ball_radius * np.sqrt(1 - offset_from_center**2)\n",
    "                        depth = ball_in_camera[2] - depth_offset\n",
    "                        rgb_image[v, u] = ball_color\n",
    "                        depth_image[v, u] = depth\n",
    "    \n",
    "    return rgb_image, depth_image\n",
    "\n",
    "\n",
    "# Manipulation functions\n",
    "\n",
    "def compute_sphere_grasp(ball_center, ball_radius):\n",
    "    \"\"\"Compute top-down grasp pose for sphere\"\"\"\n",
    "    finger_depth = 0.01\n",
    "    grasp_height = ball_radius + finger_depth\n",
    "    p_WG = ball_center + np.array([0, 0, grasp_height])\n",
    "    R_WG = RotationMatrix(RollPitchYaw([np.pi, 0, 0]))\n",
    "    X_WG = RigidTransform(R_WG, p_WG)\n",
    "    return X_WG\n",
    "\n",
    "\n",
    "def solve_ik_position_priority(plant, plant_context, target_position, \n",
    "                                 orientation_target=None, pos_tol=0.005):\n",
    "    \"\"\"IK solver prioritizing position over strict orientation\"\"\"\n",
    "    ik = InverseKinematics(plant, plant_context)\n",
    "    gripper_frame = plant.GetFrameByName(\"body\")\n",
    "    \n",
    "    ik.AddPositionConstraint(\n",
    "        gripper_frame, [0, 0, 0], plant.world_frame(),\n",
    "        target_position - pos_tol, target_position + pos_tol\n",
    "    )\n",
    "    \n",
    "    if orientation_target is not None:\n",
    "        if isinstance(orientation_target, RigidTransform):\n",
    "            orientation_target = orientation_target.rotation()\n",
    "        ik.AddOrientationConstraint(\n",
    "            gripper_frame, RotationMatrix(), plant.world_frame(),\n",
    "            orientation_target, 0.3\n",
    "        )\n",
    "    \n",
    "    q_nominal = plant.GetPositions(plant_context, plant.GetModelInstanceByName(\"iiwa\"))\n",
    "    ik.prog().AddQuadraticErrorCost(np.eye(7) * 5.0, q_nominal, ik.q())\n",
    "    \n",
    "    result = Solve(ik.prog())\n",
    "    if result.is_success():\n",
    "        return True, result.GetSolution(ik.q())\n",
    "    return False, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scene built with robot, gripper, and RGB-D camera\n",
      "  Camera at [ 0.5 -0.5  0.5], looking at [ 0.  -0.6  0.1]\n",
      "  Resolution: 640x480\n"
     ]
    }
   ],
   "source": [
    "# Build scene with robot, gripper, AND camera\n",
    "\n",
    "# Robot and gripper scenario\n",
    "scenario_yaml = \"\"\"\n",
    "directives:\n",
    "- add_model:\n",
    "    name: iiwa\n",
    "    file: package://drake_models/iiwa_description/urdf/iiwa14_primitive_collision.urdf\n",
    "    default_joint_positions:\n",
    "      iiwa_joint_1: [-1.57]\n",
    "      iiwa_joint_2: [0.1]\n",
    "      iiwa_joint_3: [0]\n",
    "      iiwa_joint_4: [-1.2]\n",
    "      iiwa_joint_5: [0]\n",
    "      iiwa_joint_6: [1.6]\n",
    "      iiwa_joint_7: [0]\n",
    "- add_weld:\n",
    "    parent: world\n",
    "    child: iiwa::iiwa_link_0\n",
    "- add_model:\n",
    "    name: wsg\n",
    "    file: package://drake_models/wsg_50_description/sdf/schunk_wsg_50_welded_fingers.sdf\n",
    "- add_weld:\n",
    "    parent: iiwa::iiwa_link_7\n",
    "    child: wsg::body\n",
    "    X_PC:\n",
    "      translation: [0, 0, 0.09]\n",
    "      rotation: !Rpy { deg: [90, 0, 90] }\n",
    "\"\"\"\n",
    "\n",
    "scenario = LoadScenario(data=scenario_yaml)\n",
    "builder = DiagramBuilder()\n",
    "\n",
    "# Add hardware station\n",
    "station = MakeHardwareStation(scenario, meshcat=meshcat)\n",
    "builder.AddSystem(station)\n",
    "\n",
    "# Camera setup using CameraConfig\n",
    "width, height = 640, 480\n",
    "\n",
    "camera_config = CameraConfig()\n",
    "camera_config.width = width\n",
    "camera_config.height = height\n",
    "camera_config.fps = 10.0\n",
    "camera_config.renderer_name = \"my_renderer\"\n",
    "\n",
    "# Camera pose: looking at bin area\n",
    "camera_pos = np.array([0.5, -0.5, 0.5])\n",
    "look_at_pos = np.array([0.0, -0.6, 0.1])\n",
    "\n",
    "forward = look_at_pos - camera_pos\n",
    "forward = forward / np.linalg.norm(forward)\n",
    "world_up = np.array([0, 0, 1])\n",
    "right = np.cross(forward, world_up)\n",
    "right = right / np.linalg.norm(right)\n",
    "down = np.cross(forward, right)\n",
    "\n",
    "R_WorldToCamera = RotationMatrix(np.column_stack([right, down, forward]))\n",
    "X_WorldToCamera = RigidTransform(R_WorldToCamera, camera_pos)\n",
    "\n",
    "# Create color and depth cameras from config\n",
    "color_camera, depth_camera = camera_config.MakeCameras()\n",
    "\n",
    "# Get scene graph from station (for frame id)\n",
    "scene_graph = station.GetSubsystemByName(\"scene_graph\")\n",
    "\n",
    "# Add RgbdSensor to diagram\n",
    "rgbd_sensor = builder.AddSystem(RgbdSensor(\n",
    "    parent_id=scene_graph.world_frame_id(),\n",
    "    X_PB=X_WorldToCamera,\n",
    "    color_camera=color_camera,\n",
    "    depth_camera=depth_camera\n",
    "))\n",
    "\n",
    "# Connect using station's query output port (not scene_graph directly)\n",
    "builder.Connect(\n",
    "    station.GetOutputPort(\"query_object\"),\n",
    "    rgbd_sensor.query_object_input_port()\n",
    ")\n",
    "\n",
    "# Build complete diagram\n",
    "diagram = builder.Build()\n",
    "\n",
    "# Initialize simulation\n",
    "simulator = Simulator(diagram)\n",
    "simulator.set_target_realtime_rate(1.0)\n",
    "context = simulator.get_mutable_context()\n",
    "\n",
    "plant = station.GetSubsystemByName(\"plant\")\n",
    "plant_context = plant.GetMyContextFromRoot(context)\n",
    "\n",
    "# Home position\n",
    "q_home = np.array([0, 0.3, 0, -1.5, 0, 1.2, 0])\n",
    "plant.SetPositions(plant_context, plant.GetModelInstanceByName(\"iiwa\"), q_home)\n",
    "\n",
    "simulator.AdvanceTo(0.1)\n",
    "\n",
    "# Visualize camera\n",
    "from pydrake.geometry import Cylinder\n",
    "meshcat.SetObject(\"camera/body\", Box(0.05, 0.05, 0.08), Rgba(0.2, 0.2, 0.2, 0.8))\n",
    "meshcat.SetTransform(\"camera/body\", X_WorldToCamera)\n",
    "meshcat.SetObject(\"camera/lens\", Cylinder(0.02, 0.03), Rgba(0.1, 0.1, 0.3, 0.9))\n",
    "meshcat.SetTransform(\"camera/lens\", X_WorldToCamera @ RigidTransform([0, 0, 0.05]))\n",
    "\n",
    "# Save camera info for perception\n",
    "camera_info = color_camera.core().intrinsics()\n",
    "\n",
    "print(\"Scene built with robot, gripper, and RGB-D camera\")\n",
    "print(f\"  Camera at {camera_pos}, looking at {look_at_pos}\")\n",
    "print(f\"  Resolution: {width}x{height}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bin and balls added to scene\n",
      "  Red ball at: [ 0.05 -0.55  0.04]\n",
      "  Blue ball at: [-0.05, -0.65, 0.04]\n"
     ]
    }
   ],
   "source": [
    "# Add bin and balls to scene (Meshcat visualization)\n",
    "\n",
    "bin_x, bin_y = 0, -0.6\n",
    "bin_w, bin_h, bin_d = 0.25, 0.25, 0.08\n",
    "wall_thickness = 0.01\n",
    "ball_radius = 0.05\n",
    "\n",
    "# Bin walls\n",
    "bin_color = Rgba(0.6, 0.4, 0.3, 0.6)\n",
    "meshcat.SetObject(\"bin/floor\", Box(bin_w, bin_h, wall_thickness), bin_color)\n",
    "meshcat.SetTransform(\"bin/floor\", RigidTransform([bin_x, bin_y, 0.0]))\n",
    "\n",
    "meshcat.SetObject(\"bin/front\", Box(bin_w, wall_thickness, bin_d), bin_color)\n",
    "meshcat.SetTransform(\"bin/front\", RigidTransform([bin_x, bin_y - bin_h/2, bin_d/2]))\n",
    "\n",
    "meshcat.SetObject(\"bin/back\", Box(bin_w, wall_thickness, bin_d), bin_color)\n",
    "meshcat.SetTransform(\"bin/back\", RigidTransform([bin_x, bin_y + bin_h/2, bin_d/2]))\n",
    "\n",
    "meshcat.SetObject(\"bin/left\", Box(wall_thickness, bin_h, bin_d), bin_color)\n",
    "meshcat.SetTransform(\"bin/left\", RigidTransform([bin_x - bin_w/2, bin_y, bin_d/2]))\n",
    "\n",
    "meshcat.SetObject(\"bin/right\", Box(wall_thickness, bin_h, bin_d), bin_color)\n",
    "meshcat.SetTransform(\"bin/right\", RigidTransform([bin_x + bin_w/2, bin_y, bin_d/2]))\n",
    "\n",
    "# Blue ball\n",
    "blue_positions = [[-0.05, -0.65, 0.04]]\n",
    "for i, pos in enumerate(blue_positions):\n",
    "    meshcat.SetObject(f\"balls/blue_{i}\", Sphere(ball_radius), Rgba(0.2, 0.4, 0.9, 1.0))\n",
    "    meshcat.SetTransform(f\"balls/blue_{i}\", RigidTransform(pos))\n",
    "\n",
    "# Red target ball\n",
    "red_pos = np.array([0.05, -0.55, 0.04])\n",
    "meshcat.SetObject(\"balls/red_target\", Sphere(ball_radius), Rgba(0.9, 0.1, 0.1, 1.0))\n",
    "meshcat.SetTransform(\"balls/red_target\", RigidTransform(red_pos))\n",
    "\n",
    "print(f\"Bin and balls added to scene\")\n",
    "print(f\"  Red ball at: {red_pos}\")\n",
    "print(f\"  Blue ball at: {blue_positions[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CAMERA PERCEPTION PIPELINE ===\n",
      "\n",
      "Step 1: Captured RGB-D images\n",
      "  RGB: (480, 640, 3), Depth: (480, 640)\n",
      "  Depth range: [0.59, 0.73]m\n",
      "\n",
      "Step 2: Generated 11107 points in camera frame\n",
      "\n",
      "Step 3: Transformed to world coordinates\n",
      "\n",
      "Step 4: Color segmentation found 6346 red points\n",
      "\n",
      "Step 5: Ball center computed\n",
      "  Detected position: [ 0.07347697 -0.54741826  0.06413159]\n",
      "  Ground truth:      [ 0.05 -0.55  0.04]\n",
      "  Error: 33.8mm\n",
      "\n",
      "============================================================\n",
      "RED BALL DETECTED VIA CAMERA PERCEPTION\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=== CAMERA PERCEPTION PIPELINE ===\\n\")\n",
    "\n",
    "# Simulate camera capture (since balls aren't in scene graph yet)\n",
    "# TODO: To use real RgbdSensor, add balls to scene graph via scenario YAML\n",
    "all_ball_positions = [red_pos] + blue_positions\n",
    "all_ball_colors = [\n",
    "    np.array([230, 25, 25]),  # Red\n",
    "    np.array([50, 100, 230]),  # Blue\n",
    "]\n",
    "\n",
    "rgb_img, depth_img = simulate_rgbd_capture(\n",
    "    camera_info, X_WorldToCamera, \n",
    "    all_ball_positions, all_ball_colors, \n",
    "    ball_radius\n",
    ")\n",
    "\n",
    "print(f\"Step 1: Captured RGB-D images\")\n",
    "print(f\"  RGB: {rgb_img.shape}, Depth: {depth_img.shape}\")\n",
    "print(f\"  Depth range: [{depth_img[depth_img>0].min():.2f}, {depth_img[depth_img>0].max():.2f}]m\\n\")\n",
    "\n",
    "# Step 2: Convert to point cloud\n",
    "points_camera, colors = depth_image_to_point_cloud(depth_img, rgb_img, camera_info)\n",
    "print(f\"Step 2: Generated {len(points_camera)} points in camera frame\\n\")\n",
    "\n",
    "# Step 3: Transform to world frame\n",
    "points_world = transform_points_to_world(points_camera, X_WorldToCamera)\n",
    "print(f\"Step 3: Transformed to world coordinates\\n\")\n",
    "\n",
    "# Step 4: Color segmentation - find red points\n",
    "red_points = segment_red_color(points_world, colors, red_min=0.6, other_max=0.4)\n",
    "print(f\"Step 4: Color segmentation found {len(red_points)} red points\\n\")\n",
    "\n",
    "# Step 5: Compute ball center\n",
    "detected_red_pos = compute_ball_center(red_points)\n",
    "\n",
    "if detected_red_pos is not None:\n",
    "    error = np.linalg.norm(detected_red_pos - red_pos)\n",
    "    print(f\"Step 5: Ball center computed\")\n",
    "    print(f\"  Detected position: {detected_red_pos}\")\n",
    "    print(f\"  Ground truth:      {red_pos}\")\n",
    "    print(f\"  Error: {error*1000:.1f}mm\\n\")\n",
    "    \n",
    "    # Visualize detection\n",
    "    meshcat.SetObject(\"perception/detected\", Sphere(0.055), Rgba(1.0, 0.5, 0.0, 0.6))\n",
    "    meshcat.SetTransform(\"perception/detected\", RigidTransform(detected_red_pos))\n",
    "    \n",
    "    # Visualize red point cloud (downsample)\n",
    "    for i in range(0, len(red_points), max(1, len(red_points)//50)):\n",
    "        meshcat.SetObject(f\"perception/red_pt_{i}\", Sphere(0.003), Rgba(1, 0, 0, 0.8))\n",
    "        meshcat.SetTransform(f\"perception/red_pt_{i}\", RigidTransform(red_points[i]))\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"RED BALL DETECTED VIA CAMERA PERCEPTION\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"✗ No red ball detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MANIPULATION: Pick Detected Ball ===\n",
      "\n",
      "Target ball position: [ 0.07347697 -0.54741826  0.06413159]\n",
      "\n",
      "Step 0: Moving to HOME position...\n",
      "  ✓ At home\n",
      "\n",
      "Planning waypoints:\n",
      "  ✓ Pre-grasp at [ 0.07347697 -0.54741826  0.21413159]\n",
      "  ✓ Grasp at [ 0.07347697 -0.54741826  0.12413159]\n",
      "  ✓ Lift at [ 0.07347697 -0.54741826  0.26413159]\n",
      "  ✗ Pre-throw at [ 0.07347697 -0.54741826  0.41413159]\n",
      "\n",
      "✓ Planned 3/4 waypoints\n",
      "\n",
      "=== Executing motion sequence ===\n",
      "\n",
      "  → Moving to Pre-grasp...\n",
      "    ✓ Pre-grasp\n",
      "  → Moving to Grasp...\n",
      "    ✓ Grasp\n",
      "  → Moving to Lift...\n",
      "    ✓ Lift\n",
      "\n",
      "============================================================\n",
      "SUCCESS! PERCEPTION + MANIPULATION COMPLETE\n",
      "============================================================\n",
      "Used camera perception to detect and grasp red ball\n",
      "Total duration: 4.1s\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# MANIPULATION: Pick the detected red ball\n",
    "\n",
    "if detected_red_pos is None:\n",
    "    print(\"ERROR: No ball detected, cannot proceed with manipulation\")\n",
    "else:\n",
    "    print(\"=== MANIPULATION: Pick Detected Ball ===\\n\")\n",
    "    \n",
    "    # Use perception-detected position\n",
    "    target_pos = detected_red_pos\n",
    "    print(f\"Target ball position: {target_pos}\\n\")\n",
    "    \n",
    "    # Move to home first\n",
    "    print(\"Step 0: Moving to HOME position...\")\n",
    "    plant.SetPositions(plant_context, plant.GetModelInstanceByName(\"iiwa\"), q_home)\n",
    "    diagram.ForcedPublish(context)\n",
    "    time.sleep(0.3)\n",
    "    print(\"  ✓ At home\\n\")\n",
    "    \n",
    "    # Compute grasp pose\n",
    "    X_WG_grasp = compute_sphere_grasp(target_pos, ball_radius)\n",
    "    \n",
    "    # Plan waypoints\n",
    "    print(\"Planning waypoints:\")\n",
    "    \n",
    "    # Waypoint 1: Pre-grasp (15cm above ball)\n",
    "    pre_grasp_pos = target_pos + np.array([0, 0, 0.15])\n",
    "    success_pre, q_pregrasp = solve_ik_position_priority(\n",
    "        plant, plant_context, pre_grasp_pos, \n",
    "        orientation_target=X_WG_grasp.rotation(), pos_tol=0.005\n",
    "    )\n",
    "    \n",
    "    # Waypoint 2: Grasp (at ball)\n",
    "    grasp_pos = X_WG_grasp.translation()\n",
    "    success_grasp, q_grasp = solve_ik_position_priority(\n",
    "        plant, plant_context, grasp_pos, \n",
    "        orientation_target=X_WG_grasp.rotation(), pos_tol=0.003\n",
    "    )\n",
    "    \n",
    "    # Waypoint 3: Lift (20cm above ball)\n",
    "    lift_pos = target_pos + np.array([0, 0, 0.20])\n",
    "    success_lift, q_lift = solve_ik_position_priority(\n",
    "        plant, plant_context, lift_pos,\n",
    "        orientation_target=X_WG_grasp.rotation(), pos_tol=0.005\n",
    "    )\n",
    "    \n",
    "    # Waypoint 4: Pre-throw\n",
    "    prethrow_pos = lift_pos + np.array([0, 0, 0.15])\n",
    "    success_prethrow, q_prethrow = solve_ik_position_priority(\n",
    "        plant, plant_context, prethrow_pos,\n",
    "        orientation_target=X_WG_grasp.rotation(), pos_tol=0.01\n",
    "    )\n",
    "    \n",
    "    # Build waypoint list\n",
    "    waypoints = []\n",
    "    waypoint_names = []\n",
    "    successes = [success_pre, success_grasp, success_lift, success_prethrow]\n",
    "    names = [\"Pre-grasp\", \"Grasp\", \"Lift\", \"Pre-throw\"]\n",
    "    qs = [q_pregrasp, q_grasp, q_lift, q_prethrow]\n",
    "    positions = [pre_grasp_pos, grasp_pos, lift_pos, prethrow_pos]\n",
    "    \n",
    "    for success, name, q, pos in zip(successes, names, qs, positions):\n",
    "        status = \"✓\" if success else \"✗\"\n",
    "        print(f\"  {status} {name} at {pos}\")\n",
    "        if success:\n",
    "            waypoints.append(q)\n",
    "            waypoint_names.append(name)\n",
    "    \n",
    "    print(f\"\\n✓ Planned {len(waypoints)}/4 waypoints\\n\")\n",
    "    \n",
    "    # Visualize waypoints\n",
    "    if success_pre:\n",
    "        meshcat.SetObject(\"waypoints/pregrasp\", Sphere(0.02), Rgba(1, 1, 0, 0.6))\n",
    "        meshcat.SetTransform(\"waypoints/pregrasp\", RigidTransform(pre_grasp_pos))\n",
    "    if success_grasp:\n",
    "        meshcat.SetObject(\"waypoints/grasp\", Sphere(0.02), Rgba(0, 1, 0, 0.8))\n",
    "        meshcat.SetTransform(\"waypoints/grasp\", RigidTransform(grasp_pos))\n",
    "    if success_lift:\n",
    "        meshcat.SetObject(\"waypoints/lift\", Sphere(0.02), Rgba(0, 1, 1, 0.6))\n",
    "        meshcat.SetTransform(\"waypoints/lift\", RigidTransform(lift_pos))\n",
    "    if success_prethrow:\n",
    "        meshcat.SetObject(\"waypoints/prethrow\", Sphere(0.025), Rgba(1, 0.65, 0, 1.0))\n",
    "        meshcat.SetTransform(\"waypoints/prethrow\", RigidTransform(prethrow_pos))\n",
    "    \n",
    "    # Execute motion\n",
    "    if len(waypoints) >= 3:\n",
    "        print(\"=== Executing motion sequence ===\\n\")\n",
    "        \n",
    "        meshcat.StartRecording()\n",
    "        \n",
    "        all_waypoints = [q_home] + waypoints\n",
    "        all_names = [\"Home\"] + waypoint_names\n",
    "        \n",
    "        sim_time = 0.0\n",
    "        dt = 0.03\n",
    "        \n",
    "        for i in range(len(all_waypoints) - 1):\n",
    "            q_start = all_waypoints[i]\n",
    "            q_end = all_waypoints[i + 1]\n",
    "            name = all_names[i + 1]\n",
    "            \n",
    "            print(f\"  → Moving to {name}...\")\n",
    "            \n",
    "            num_steps = 35\n",
    "            for step in range(num_steps + 1):\n",
    "                alpha = step / num_steps\n",
    "                q_interp = (1 - alpha) * q_start + alpha * q_end\n",
    "                \n",
    "                plant.SetPositions(plant_context, plant.GetModelInstanceByName(\"iiwa\"), q_interp)\n",
    "                context.SetTime(sim_time)\n",
    "                diagram.ForcedPublish(context)\n",
    "                \n",
    "                time.sleep(dt)\n",
    "                sim_time += dt\n",
    "            \n",
    "            print(f\"    ✓ {name}\")\n",
    "            time.sleep(0.3)\n",
    "            sim_time += 0.3\n",
    "        \n",
    "        meshcat.StopRecording()\n",
    "        meshcat.PublishRecording()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"SUCCESS! PERCEPTION + MANIPULATION COMPLETE\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Used camera perception to detect and grasp red ball\")\n",
    "        print(f\"Total duration: {sim_time:.1f}s\")\n",
    "        print(\"=\"*60)\n",
    "    else:\n",
    "        print(\"✗ Too few waypoints succeeded - check IK solutions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
