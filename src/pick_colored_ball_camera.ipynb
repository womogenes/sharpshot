{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports loaded\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import time\n",
    "from pydrake.all import (\n",
    "    DiagramBuilder,\n",
    "    Simulator,\n",
    "    StartMeshcat,\n",
    "    RigidTransform,\n",
    "    RotationMatrix,\n",
    "    Sphere,\n",
    "    Box,\n",
    "    RollPitchYaw,\n",
    "    InverseKinematics,\n",
    "    Solve,\n",
    "    MakeRenderEngineVtk,\n",
    "    RenderEngineVtkParams,\n",
    ")\n",
    "from pydrake.geometry import Rgba\n",
    "from pydrake.systems.sensors import RgbdSensor, CameraConfig\n",
    "from manipulation.station import LoadScenario, MakeHardwareStation\n",
    "\n",
    "print(\"✓ Imports loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:drake:Meshcat listening for connections at http://localhost:7001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meshcat: http://localhost:7001\n"
     ]
    }
   ],
   "source": [
    "meshcat = StartMeshcat()\n",
    "print(f\"Meshcat: {meshcat.web_url()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perception functions\n",
    "\n",
    "def depth_image_to_point_cloud(depth_image, rgb_image, camera_info):\n",
    "    \"\"\"Convert depth image to colored point cloud in camera frame\"\"\"\n",
    "    height, width = depth_image.shape\n",
    "    fx = camera_info.focal_x()\n",
    "    fy = camera_info.focal_y()\n",
    "    cx = camera_info.center_x()\n",
    "    cy = camera_info.center_y()\n",
    "    \n",
    "    # Pixel grid\n",
    "    u = np.arange(width)\n",
    "    v = np.arange(height)\n",
    "    u_grid, v_grid = np.meshgrid(u, v)\n",
    "    \n",
    "    # Valid depth mask\n",
    "    valid = (depth_image > 0.01) & (depth_image < 5.0)\n",
    "    \n",
    "    u_valid = u_grid[valid]\n",
    "    v_valid = v_grid[valid]\n",
    "    z_valid = depth_image[valid]\n",
    "    \n",
    "    # Back-project to 3D (camera frame: X right, Y down, Z forward)\n",
    "    x = (u_valid - cx) * z_valid / fx\n",
    "    y = (v_valid - cy) * z_valid / fy\n",
    "    z = z_valid\n",
    "    \n",
    "    points = np.column_stack([x, y, z])\n",
    "    colors = rgb_image[valid].astype(float) / 255.0\n",
    "    \n",
    "    return points, colors\n",
    "\n",
    "\n",
    "def transform_points_to_world(points, X_CameraToWorld):\n",
    "    \"\"\"Transform points from camera frame to world frame\"\"\"\n",
    "    points_world = []\n",
    "    for pt in points:\n",
    "        pt_world = X_CameraToWorld @ pt\n",
    "        points_world.append(pt_world)\n",
    "    return np.array(points_world)\n",
    "\n",
    "\n",
    "def segment_red_color(points, colors, red_min=0.5, other_max=0.35):\n",
    "    \"\"\"Segment red points using color thresholds\"\"\"\n",
    "    is_red = (\n",
    "        (colors[:, 0] > red_min) &      # R > 0.5\n",
    "        (colors[:, 1] < other_max) &    # G < 0.35\n",
    "        (colors[:, 2] < other_max)      # B < 0.35\n",
    "    )\n",
    "    return points[is_red]\n",
    "\n",
    "\n",
    "def compute_ball_center(points):\n",
    "    \"\"\"Compute centroid of point cluster\"\"\"\n",
    "    if len(points) == 0:\n",
    "        return None\n",
    "    return np.mean(points, axis=0)\n",
    "\n",
    "\n",
    "def simulate_rgbd_capture(camera_info, X_WorldToCamera, ball_positions, ball_colors, ball_radius=0.05):\n",
    "    \"\"\"Simulate RGB-D camera seeing balls (for testing without real scene graph objects)\"\"\"\n",
    "    width = camera_info.width()\n",
    "    height = camera_info.height()\n",
    "    fx = camera_info.focal_x()\n",
    "    fy = camera_info.focal_y()\n",
    "    cx = camera_info.center_x()\n",
    "    cy = camera_info.center_y()\n",
    "    \n",
    "    # Initialize images\n",
    "    rgb_image = np.ones((height, width, 3), dtype=np.uint8) * 50  # Dark background\n",
    "    depth_image = np.zeros((height, width), dtype=np.float32)\n",
    "    \n",
    "    # Transform balls to camera frame\n",
    "    X_CameraToWorld = X_WorldToCamera.inverse()\n",
    "    \n",
    "    for ball_pos, ball_color in zip(ball_positions, ball_colors):\n",
    "        ball_in_camera = X_CameraToWorld @ ball_pos\n",
    "        \n",
    "        if ball_in_camera[2] <= 0:\n",
    "            continue\n",
    "        \n",
    "        u_center = fx * ball_in_camera[0] / ball_in_camera[2] + cx\n",
    "        v_center = fy * ball_in_camera[1] / ball_in_camera[2] + cy\n",
    "        radius_px = int(fx * ball_radius / ball_in_camera[2])\n",
    "        \n",
    "        for v in range(max(0, int(v_center - radius_px)), min(height, int(v_center + radius_px))):\n",
    "            for u in range(max(0, int(u_center - radius_px)), min(width, int(u_center + radius_px))):\n",
    "                du = u - u_center\n",
    "                dv = v - v_center\n",
    "                \n",
    "                if du**2 + dv**2 < radius_px**2:\n",
    "                    offset_from_center = np.sqrt(du**2 + dv**2) / radius_px\n",
    "                    if offset_from_center < 1.0:\n",
    "                        depth_offset = ball_radius * np.sqrt(1 - offset_from_center**2)\n",
    "                        depth = ball_in_camera[2] - depth_offset\n",
    "                        rgb_image[v, u] = ball_color\n",
    "                        depth_image[v, u] = depth\n",
    "    \n",
    "    return rgb_image, depth_image\n",
    "\n",
    "\n",
    "# Manipulation functions\n",
    "\n",
    "def compute_sphere_grasp(ball_center, ball_radius):\n",
    "    \"\"\"Compute top-down grasp pose for sphere\"\"\"\n",
    "    finger_depth = 0.01\n",
    "    grasp_height = ball_radius + finger_depth\n",
    "    p_WG = ball_center + np.array([0, 0, grasp_height])\n",
    "    R_WG = RotationMatrix(RollPitchYaw([np.pi, 0, 0]))\n",
    "    X_WG = RigidTransform(R_WG, p_WG)\n",
    "    return X_WG\n",
    "\n",
    "\n",
    "def normalize_joint_angles(q_start, q_end):\n",
    "    \"\"\"Normalize joint angles to take shortest path (prevents 360 spins!)\n",
    "    \n",
    "    For each joint, if the difference is > π, adjust by 2π to take shorter path.\n",
    "    \"\"\"\n",
    "    q_end_normalized = q_end.copy()\n",
    "    for i in range(len(q_start)):\n",
    "        diff = q_end[i] - q_start[i]\n",
    "        # If difference is > π, we're going the long way around\n",
    "        if diff > np.pi:\n",
    "            q_end_normalized[i] -= 2 * np.pi\n",
    "        elif diff < -np.pi:\n",
    "            q_end_normalized[i] += 2 * np.pi\n",
    "    return q_end_normalized\n",
    "\n",
    "\n",
    "def solve_ik_position_priority(plant, plant_context, target_position, \n",
    "                                 orientation_target=None, pos_tol=0.005):\n",
    "    \"\"\"IK solver prioritizing position over strict orientation\n",
    "    \n",
    "    Note: Bin collision geometry exists in scene graph, so the robot\n",
    "    will respect it during execution even if IK doesn't explicitly check.\n",
    "    We ensure collision-free paths by approaching from above.\n",
    "    \"\"\"\n",
    "    ik = InverseKinematics(plant, plant_context)\n",
    "    gripper_frame = plant.GetFrameByName(\"body\")\n",
    "    \n",
    "    # Position constraint\n",
    "    ik.AddPositionConstraint(\n",
    "        gripper_frame, [0, 0, 0], plant.world_frame(),\n",
    "        target_position - pos_tol, target_position + pos_tol\n",
    "    )\n",
    "    \n",
    "    # Orientation constraint\n",
    "    if orientation_target is not None:\n",
    "        if isinstance(orientation_target, RigidTransform):\n",
    "            orientation_target = orientation_target.rotation()\n",
    "        ik.AddOrientationConstraint(\n",
    "            gripper_frame, RotationMatrix(), plant.world_frame(),\n",
    "            orientation_target, 0.3\n",
    "        )\n",
    "    \n",
    "    # Cost to stay near current configuration (increased to discourage large movements)\n",
    "    q_nominal = plant.GetPositions(plant_context, plant.GetModelInstanceByName(\"iiwa\"))\n",
    "    ik.prog().AddQuadraticErrorCost(np.eye(7) * 50.0, q_nominal, ik.q())  # Increased from 5.0\n",
    "    \n",
    "    result = Solve(ik.prog())\n",
    "    if result.is_success():\n",
    "        return True, result.GetSolution(ik.q())\n",
    "    return False, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:drake:Drake materials have been assigned to a glTF file. glTF defines its own materials, so post hoc materials will be ignored and should be removed from the model specification. glTF file: '/root/.cache/drake/package_map/719c542a6323d5a8cc74fe6da84f305aebc6d7810b84cb94023d80f78e06586b-6e8a5cbbb49ceed5e2ae3a392577ca12bd01fd5983dc56151cb5fa7d55d27ed8/wsg_50_description/meshes/wsg_body.gltf'\n",
      "WARNING:drake:Drake materials have been assigned to a glTF file. glTF defines its own materials, so post hoc materials will be ignored and should be removed from the model specification. glTF file: '/root/.cache/drake/package_map/719c542a6323d5a8cc74fe6da84f305aebc6d7810b84cb94023d80f78e06586b-6e8a5cbbb49ceed5e2ae3a392577ca12bd01fd5983dc56151cb5fa7d55d27ed8/wsg_50_description/meshes/finger_with_tip.gltf'\n",
      "WARNING:drake:Drake materials have been assigned to a glTF file. glTF defines its own materials, so post hoc materials will be ignored and should be removed from the model specification. glTF file: '/root/.cache/drake/package_map/719c542a6323d5a8cc74fe6da84f305aebc6d7810b84cb94023d80f78e06586b-6e8a5cbbb49ceed5e2ae3a392577ca12bd01fd5983dc56151cb5fa7d55d27ed8/wsg_50_description/meshes/finger_with_tip.gltf'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial gripper pose: [ 3.70783219e-04 -4.65616808e-01  6.79321579e-01]\n",
      "✓ PHYSICS-ENABLED SETUP: Robot + Table + FREE-BODY Balls + 4 CAMERAS\n",
      "  Resolution: 640x480 per camera\n",
      "  Red ball at: [ 0.   -0.5   0.05] (FREE BODY)\n",
      "  Blue ball at: [-0.15 -0.55  0.05] (FREE BODY)\n",
      "\n",
      "  Cameras: Cyan (front-right), Orange (back-left), Purple (left), Teal (right)\n",
      "\n",
      "✓ Ready for perception (cell 5), then physics simulation (cell 6)\n"
     ]
    }
   ],
   "source": [
    "# Build scene with robot, gripper, TABLE, and 4 CAMERAS (with PHYSICS!)\n",
    "\n",
    "import tempfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create temporary directory for SDF files\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "# Red ball SDF - with proper inertia for physics\n",
    "red_ball_sdf = \"\"\"<?xml version=\"1.0\"?>\n",
    "<sdf version=\"1.7\">\n",
    "  <model name=\"red_ball\">\n",
    "    <link name=\"ball\">\n",
    "      <inertial>\n",
    "        <mass>0.1</mass>\n",
    "        <inertia>\n",
    "          <ixx>0.0001</ixx>\n",
    "          <ixy>0</ixy>\n",
    "          <ixz>0</ixz>\n",
    "          <iyy>0.0001</iyy>\n",
    "          <iyz>0</iyz>\n",
    "          <izz>0.0001</izz>\n",
    "        </inertia>\n",
    "      </inertial>\n",
    "      <visual name=\"visual\">\n",
    "        <geometry>\n",
    "          <sphere>\n",
    "            <radius>0.05</radius>\n",
    "          </sphere>\n",
    "        </geometry>\n",
    "        <material>\n",
    "          <diffuse>0.9 0.1 0.1 1.0</diffuse>\n",
    "        </material>\n",
    "      </visual>\n",
    "      <collision name=\"collision\">\n",
    "        <geometry>\n",
    "          <sphere>\n",
    "            <radius>0.05</radius>\n",
    "          </sphere>\n",
    "        </geometry>\n",
    "      </collision>\n",
    "    </link>\n",
    "  </model>\n",
    "</sdf>\n",
    "\"\"\"\n",
    "\n",
    "# Blue ball SDF\n",
    "blue_ball_sdf = \"\"\"<?xml version=\"1.0\"?>\n",
    "<sdf version=\"1.7\">\n",
    "  <model name=\"blue_ball\">\n",
    "    <link name=\"ball\">\n",
    "      <inertial>\n",
    "        <mass>0.1</mass>\n",
    "        <inertia>\n",
    "          <ixx>0.0001</ixx>\n",
    "          <ixy>0</ixy>\n",
    "          <ixz>0</ixz>\n",
    "          <iyy>0.0001</iyy>\n",
    "          <iyz>0</iyz>\n",
    "          <izz>0.0001</izz>\n",
    "        </inertia>\n",
    "      </inertial>\n",
    "      <visual name=\"visual\">\n",
    "        <geometry>\n",
    "          <sphere>\n",
    "            <radius>0.05</radius>\n",
    "          </sphere>\n",
    "        </geometry>\n",
    "        <material>\n",
    "          <diffuse>0.2 0.4 0.9 1.0</diffuse>\n",
    "        </material>\n",
    "      </visual>\n",
    "      <collision name=\"collision\">\n",
    "        <geometry>\n",
    "          <sphere>\n",
    "            <radius>0.05</radius>\n",
    "          </sphere>\n",
    "        </geometry>\n",
    "      </collision>\n",
    "    </link>\n",
    "  </model>\n",
    "</sdf>\n",
    "\"\"\"\n",
    "\n",
    "# Write SDF files\n",
    "red_ball_path = os.path.join(temp_dir, \"red_ball.sdf\")\n",
    "blue_ball_path = os.path.join(temp_dir, \"blue_ball.sdf\")\n",
    "\n",
    "with open(red_ball_path, 'w') as f:\n",
    "    f.write(red_ball_sdf)\n",
    "    \n",
    "with open(blue_ball_path, 'w') as f:\n",
    "    f.write(blue_ball_sdf)\n",
    "\n",
    "# Ball positions - on TABLE surface (table top at z=0, ball radius=0.05)\n",
    "ball_radius = 0.05\n",
    "red_pos = np.array([0.0, -0.5, ball_radius])   # On table, in front of robot\n",
    "blue_pos = np.array([-0.15, -0.55, ball_radius])  # Slightly to the left\n",
    "\n",
    "# Table SDF path (from assets)\n",
    "abs_table_sdf_path = f\"{Path.cwd()}/assets/table.sdf\"\n",
    "\n",
    "# Robot and gripper scenario WITH TABLE AND FREE-BODY BALLS (physics enabled!)\n",
    "# KEY: Using default_free_body_pose instead of add_weld for balls\n",
    "scenario_yaml = f\"\"\"\n",
    "directives:\n",
    "- add_model:\n",
    "    name: iiwa\n",
    "    file: package://drake_models/iiwa_description/sdf/iiwa7_no_collision.sdf\n",
    "    default_joint_positions:\n",
    "      iiwa_joint_1: [-1.57]\n",
    "      iiwa_joint_2: [0.1]\n",
    "      iiwa_joint_3: [0]\n",
    "      iiwa_joint_4: [-1.2]\n",
    "      iiwa_joint_5: [0]\n",
    "      iiwa_joint_6: [1.6]\n",
    "      iiwa_joint_7: [0]\n",
    "- add_weld:\n",
    "    parent: world\n",
    "    child: iiwa::iiwa_link_0\n",
    "- add_model:\n",
    "    name: wsg\n",
    "    file: package://manipulation/hydro/schunk_wsg_50_with_tip.sdf\n",
    "- add_weld:\n",
    "    parent: iiwa::iiwa_link_7\n",
    "    child: wsg::body\n",
    "    X_PC:\n",
    "      translation: [0, 0, 0.09]\n",
    "      rotation: !Rpy {{ deg: [90, 0, 90] }}\n",
    "- add_model:\n",
    "    name: table\n",
    "    file: file://{abs_table_sdf_path}\n",
    "- add_weld:\n",
    "    parent: world\n",
    "    child: table::table_link\n",
    "    X_PC:\n",
    "      translation: [0.0, 0.0, -0.05]\n",
    "      rotation: !Rpy {{ deg: [0, 0, -90] }}\n",
    "- add_model:\n",
    "    name: red_ball\n",
    "    file: file://{red_ball_path}\n",
    "    default_free_body_pose:\n",
    "      ball:\n",
    "        translation: [{red_pos[0]}, {red_pos[1]}, {red_pos[2]}]\n",
    "- add_model:\n",
    "    name: blue_ball\n",
    "    file: file://{blue_ball_path}\n",
    "    default_free_body_pose:\n",
    "      ball:\n",
    "        translation: [{blue_pos[0]}, {blue_pos[1]}, {blue_pos[2]}]\n",
    "\n",
    "model_drivers:\n",
    "    iiwa: !IiwaDriver\n",
    "      control_mode: position_only\n",
    "      hand_model_name: wsg\n",
    "    wsg: !SchunkWsgDriver {{}}\n",
    "\"\"\"\n",
    "\n",
    "meshcat.Delete()\n",
    "scenario = LoadScenario(data=scenario_yaml)\n",
    "builder = DiagramBuilder()\n",
    "\n",
    "# Add hardware station\n",
    "station = MakeHardwareStation(scenario, meshcat=meshcat)\n",
    "builder.AddSystem(station)\n",
    "\n",
    "# Get plant and scene graph\n",
    "plant = station.GetSubsystemByName(\"plant\")\n",
    "scene_graph = station.GetSubsystemByName(\"scene_graph\")\n",
    "\n",
    "# Get initial poses BEFORE building diagram (like arc_trajectory_planning.ipynb cell 3)\n",
    "temp_context = station.CreateDefaultContext()\n",
    "temp_plant_context = plant.GetMyContextFromRoot(temp_context)\n",
    "\n",
    "X_WG_initial = plant.EvalBodyPoseInWorld(temp_plant_context, plant.GetBodyByName(\"body\"))\n",
    "print(f\"Initial gripper pose: {X_WG_initial.translation()}\")\n",
    "\n",
    "# Add VTK renderer to scene graph\n",
    "renderer_name = \"my_renderer\"\n",
    "params = RenderEngineVtkParams()\n",
    "renderer = MakeRenderEngineVtk(params)\n",
    "scene_graph.AddRenderer(renderer_name, renderer)\n",
    "\n",
    "# Camera setup using CameraConfig\n",
    "width, height = 640, 480\n",
    "\n",
    "camera_config = CameraConfig()\n",
    "camera_config.width = width\n",
    "camera_config.height = height\n",
    "camera_config.fps = 10.0\n",
    "camera_config.renderer_name = renderer_name\n",
    "\n",
    "# Helper function to create camera transform\n",
    "def make_camera_transform(camera_pos, look_at_pos):\n",
    "    \"\"\"Create camera transform that looks from camera_pos to look_at_pos\"\"\"\n",
    "    forward = look_at_pos - camera_pos\n",
    "    forward = forward / np.linalg.norm(forward)\n",
    "    world_up = np.array([0, 0, 1])\n",
    "    right = np.cross(forward, world_up)\n",
    "    right = right / np.linalg.norm(right)\n",
    "    down = np.cross(forward, right)\n",
    "    R_WorldToCamera = RotationMatrix(np.column_stack([right, down, forward]))\n",
    "    return RigidTransform(R_WorldToCamera, camera_pos)\n",
    "\n",
    "# Define 4 camera positions SPREAD AROUND the workspace\n",
    "workspace_center = np.array([0.0, -0.5, 0.05])\n",
    "\n",
    "camera_1_pos = np.array([0.4, -0.15, 0.5])      # Front-right, HIGH\n",
    "camera_2_pos = np.array([-0.35, -0.9, 0.25])    # Back-left, LOW\n",
    "camera_3_pos = np.array([-0.5, -0.4, 0.4])      # Left side, MEDIUM\n",
    "camera_4_pos = np.array([0.5, -0.6, 0.35])      # RIGHT SIDE\n",
    "\n",
    "X_WorldToCamera_1 = make_camera_transform(camera_1_pos, workspace_center)\n",
    "X_WorldToCamera_2 = make_camera_transform(camera_2_pos, workspace_center)\n",
    "X_WorldToCamera_3 = make_camera_transform(camera_3_pos, workspace_center)\n",
    "X_WorldToCamera_4 = make_camera_transform(camera_4_pos, workspace_center)\n",
    "\n",
    "# Create 4 sets of cameras\n",
    "color_camera_1, depth_camera_1 = camera_config.MakeCameras()\n",
    "color_camera_2, depth_camera_2 = camera_config.MakeCameras()\n",
    "color_camera_3, depth_camera_3 = camera_config.MakeCameras()\n",
    "color_camera_4, depth_camera_4 = camera_config.MakeCameras()\n",
    "\n",
    "# Add 4 RgbdSensors to diagram\n",
    "rgbd_sensor_1 = builder.AddSystem(RgbdSensor(\n",
    "    parent_id=scene_graph.world_frame_id(),\n",
    "    X_PB=X_WorldToCamera_1,\n",
    "    color_camera=color_camera_1,\n",
    "    depth_camera=depth_camera_1\n",
    "))\n",
    "\n",
    "rgbd_sensor_2 = builder.AddSystem(RgbdSensor(\n",
    "    parent_id=scene_graph.world_frame_id(),\n",
    "    X_PB=X_WorldToCamera_2,\n",
    "    color_camera=color_camera_2,\n",
    "    depth_camera=depth_camera_2\n",
    "))\n",
    "\n",
    "rgbd_sensor_3 = builder.AddSystem(RgbdSensor(\n",
    "    parent_id=scene_graph.world_frame_id(),\n",
    "    X_PB=X_WorldToCamera_3,\n",
    "    color_camera=color_camera_3,\n",
    "    depth_camera=depth_camera_3\n",
    "))\n",
    "\n",
    "rgbd_sensor_4 = builder.AddSystem(RgbdSensor(\n",
    "    parent_id=scene_graph.world_frame_id(),\n",
    "    X_PB=X_WorldToCamera_4,\n",
    "    color_camera=color_camera_4,\n",
    "    depth_camera=depth_camera_4\n",
    "))\n",
    "\n",
    "# Connect all 4 sensors to scene graph\n",
    "builder.Connect(station.GetOutputPort(\"query_object\"), rgbd_sensor_1.query_object_input_port())\n",
    "builder.Connect(station.GetOutputPort(\"query_object\"), rgbd_sensor_2.query_object_input_port())\n",
    "builder.Connect(station.GetOutputPort(\"query_object\"), rgbd_sensor_3.query_object_input_port())\n",
    "builder.Connect(station.GetOutputPort(\"query_object\"), rgbd_sensor_4.query_object_input_port())\n",
    "\n",
    "# Build diagram and publish to meshcat\n",
    "diagram = builder.Build()\n",
    "context = diagram.CreateDefaultContext()\n",
    "diagram.ForcedPublish(context)\n",
    "\n",
    "# Home position\n",
    "q_home = np.array([-1.57, 0.1, 0, -1.2, 0, 1.6, 0])\n",
    "\n",
    "# Visualize all 4 cameras in meshcat\n",
    "from pydrake.geometry import Cylinder\n",
    "\n",
    "camera_colors = [\n",
    "    Rgba(0.0, 0.5, 1.0, 0.8),   # Cyan\n",
    "    Rgba(1.0, 0.5, 0.0, 0.8),   # Orange\n",
    "    Rgba(0.8, 0.0, 0.8, 0.8),   # Purple\n",
    "    Rgba(0.0, 1.0, 0.5, 0.8),   # Teal\n",
    "]\n",
    "\n",
    "camera_transforms = [X_WorldToCamera_1, X_WorldToCamera_2, X_WorldToCamera_3, X_WorldToCamera_4]\n",
    "camera_positions = [camera_1_pos, camera_2_pos, camera_3_pos, camera_4_pos]\n",
    "\n",
    "for i, (X_cam, pos, color) in enumerate(zip(camera_transforms, camera_positions, camera_colors), 1):\n",
    "    meshcat.SetObject(f\"camera{i}/body\", Box(0.05, 0.05, 0.08), color)\n",
    "    meshcat.SetTransform(f\"camera{i}/body\", X_cam)\n",
    "    \n",
    "    lens_color = Rgba(color.r() * 0.5, color.g() * 0.5, color.b() * 0.5, 0.9)\n",
    "    meshcat.SetObject(f\"camera{i}/lens\", Cylinder(0.02, 0.03), lens_color)\n",
    "    meshcat.SetTransform(f\"camera{i}/lens\", X_cam @ RigidTransform([0, 0, 0.05]))\n",
    "\n",
    "# Save camera info and sensors for perception\n",
    "camera_info_1 = color_camera_1.core().intrinsics()\n",
    "camera_info_2 = color_camera_2.core().intrinsics()\n",
    "camera_info_3 = color_camera_3.core().intrinsics()\n",
    "camera_info_4 = color_camera_4.core().intrinsics()\n",
    "\n",
    "camera_sensors = [rgbd_sensor_1, rgbd_sensor_2, rgbd_sensor_3, rgbd_sensor_4]\n",
    "camera_infos = [camera_info_1, camera_info_2, camera_info_3, camera_info_4]\n",
    "camera_transforms = [X_WorldToCamera_1, X_WorldToCamera_2, X_WorldToCamera_3, X_WorldToCamera_4]\n",
    "\n",
    "print(\"✓ PHYSICS-ENABLED SETUP: Robot + Table + FREE-BODY Balls + 4 CAMERAS\")\n",
    "print(f\"  Resolution: {width}x{height} per camera\")\n",
    "print(f\"  Red ball at: {red_pos} (FREE BODY)\")\n",
    "print(f\"  Blue ball at: {blue_pos} (FREE BODY)\")\n",
    "print(f\"\\n  Cameras: Cyan (front-right), Orange (back-left), Purple (left), Teal (right)\")\n",
    "print(f\"\\n✓ Ready for perception (cell 5), then physics simulation (cell 6)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Scene ready for perception\n",
      "  Red ball ground truth: [ 0.   -0.5   0.05]\n",
      "  Blue ball ground truth: [-0.15 -0.55  0.05]\n"
     ]
    }
   ],
   "source": [
    "# Visualization helpers (no bin in throwing pipeline)\n",
    "meshcat.SetObject(\"markers/red_target\", Sphere(0.01), Rgba(1, 0.5, 0, 0.8))\n",
    "meshcat.SetTransform(\"markers/red_target\", RigidTransform(red_pos))\n",
    "\n",
    "meshcat.SetObject(\"markers/blue_target\", Sphere(0.01), Rgba(0, 0.5, 1, 0.8))\n",
    "meshcat.SetTransform(\"markers/blue_target\", RigidTransform(blue_pos))\n",
    "\n",
    "print(\"✓ Scene ready for perception\")\n",
    "print(f\"  Red ball ground truth: {red_pos}\")\n",
    "print(f\"  Blue ball ground truth: {blue_pos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MULTI-CAMERA PERCEPTION PIPELINE ===\n",
      "\n",
      "--- Camera 1 ---\n",
      "  Generated 266226 points\n",
      "--- Camera 2 ---\n",
      "  Generated 246300 points\n",
      "--- Camera 3 ---\n",
      "  Generated 270377 points\n",
      "--- Camera 4 ---\n",
      "  Generated 270942 points\n",
      "\n",
      "✓ Merged 1053845 points from 4 cameras\n",
      "\n",
      "--- Visualizing Point Clouds (color-coded by camera) ---\n",
      "  Camera 1: plotting 887 points (Cyan)\n",
      "  Camera 2: plotting 821 points (Orange)\n",
      "  Camera 3: plotting 901 points (Purple)\n",
      "  Camera 4: plotting 903 points (Teal)\n",
      "  ✓ Plotted ~3512 points total\n",
      "  Legend: Cyan=Cam1, Orange=Cam2, Purple=Cam3, Teal=Cam4\n",
      "\n",
      "--- Ball Detection ---\n",
      "Points above table (z > 0.02): 83671\n",
      "Reddest 4183 elevated points selected\n",
      "After clustering: 4244 points\n",
      "\n",
      "Detected: [-0.00219639 -0.50580673  0.05      ]\n",
      "Ground truth: [ 0.   -0.5   0.05]\n",
      "Error: 6.2mm\n",
      "\n",
      "============================================================\n",
      "✓ DETECTED RED BALL - Error: 6.2mm\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# PERCEPTION: Multi-camera capture and point cloud fusion (4 CAMERAS)\n",
    "\n",
    "print(\"=== MULTI-CAMERA PERCEPTION PIPELINE ===\\n\")\n",
    "\n",
    "# Lists to store point clouds from all cameras\n",
    "all_points_world = []\n",
    "all_colors = []\n",
    "\n",
    "# Process each camera\n",
    "for cam_idx, (sensor, cam_info, X_cam) in enumerate(zip(camera_sensors, camera_infos, camera_transforms), 1):\n",
    "    print(f\"--- Camera {cam_idx} ---\")\n",
    "    \n",
    "    # Get images from this sensor\n",
    "    sensor_context = sensor.GetMyContextFromRoot(context)\n",
    "    \n",
    "    color_image = sensor.color_image_output_port().Eval(sensor_context)\n",
    "    rgb_img = color_image.data[:, :, :3].astype(np.uint8)\n",
    "    \n",
    "    depth_image = sensor.depth_image_32F_output_port().Eval(sensor_context)\n",
    "    depth_img = depth_image.data.squeeze().astype(np.float32)\n",
    "    \n",
    "    # Convert to point cloud in camera frame\n",
    "    points_camera, colors = depth_image_to_point_cloud(depth_img, rgb_img, cam_info)\n",
    "    print(f\"  Generated {len(points_camera)} points\")\n",
    "    \n",
    "    # Transform to world frame\n",
    "    points_world = transform_points_to_world(points_camera, X_cam)\n",
    "    \n",
    "    all_points_world.append(points_world)\n",
    "    all_colors.append(colors)\n",
    "\n",
    "# Merge all point clouds\n",
    "merged_points = np.vstack(all_points_world)\n",
    "merged_colors = np.vstack(all_colors)\n",
    "\n",
    "print(f\"\\n✓ Merged {len(merged_points)} points from {len(camera_sensors)} cameras\\n\")\n",
    "\n",
    "# VISUALIZE point clouds from each camera with DIFFERENT COLORS\n",
    "print(\"--- Visualizing Point Clouds (color-coded by camera) ---\")\n",
    "cam_viz_colors = [\n",
    "    Rgba(0.0, 0.5, 1.0, 0.9),   # Cyan - Camera 1\n",
    "    Rgba(1.0, 0.5, 0.0, 0.9),   # Orange - Camera 2  \n",
    "    Rgba(0.8, 0.0, 0.8, 0.9),   # Purple - Camera 3\n",
    "    Rgba(0.0, 1.0, 0.5, 0.9),   # Teal - Camera 4\n",
    "]\n",
    "cam_names = ['Cyan', 'Orange', 'Purple', 'Teal']\n",
    "\n",
    "# Downsample for visualization\n",
    "downsample = 300\n",
    "\n",
    "for cam_idx, (points, viz_color) in enumerate(zip(all_points_world, cam_viz_colors)):\n",
    "    print(f\"  Camera {cam_idx+1}: plotting {len(points)//downsample} points ({cam_names[cam_idx]})\")\n",
    "    for i in range(0, len(points), downsample):\n",
    "        meshcat.SetObject(f\"pointcloud/cam{cam_idx+1}/pt_{i}\", Sphere(0.004), viz_color)\n",
    "        meshcat.SetTransform(f\"pointcloud/cam{cam_idx+1}/pt_{i}\", RigidTransform(points[i]))\n",
    "\n",
    "print(f\"  ✓ Plotted ~{len(merged_points)//downsample} points total\")\n",
    "print(f\"  Legend: Cyan=Cam1, Orange=Cam2, Purple=Cam3, Teal=Cam4\\n\")\n",
    "\n",
    "# BALL DETECTION\n",
    "print(\"--- Ball Detection ---\")\n",
    "\n",
    "# Step 1: Find points that are ABOVE the table (z > 0.02)\n",
    "above_table_mask = merged_points[:, 2] > 0.02\n",
    "above_table_points = merged_points[above_table_mask]\n",
    "above_table_colors = merged_colors[above_table_mask]\n",
    "print(f\"Points above table (z > 0.02): {len(above_table_points)}\")\n",
    "\n",
    "# Step 2: Among elevated points, find the reddest ones\n",
    "if len(above_table_points) > 0:\n",
    "    redness = above_table_colors[:, 0] - np.maximum(above_table_colors[:, 1], above_table_colors[:, 2])\n",
    "    \n",
    "    num_red_points = max(100, int(len(above_table_points) * 0.05))\n",
    "    red_threshold = np.sort(redness)[-num_red_points]\n",
    "    \n",
    "    is_red = redness >= red_threshold\n",
    "    red_points = above_table_points[is_red]\n",
    "    print(f\"Reddest {num_red_points} elevated points selected\")\n",
    "    \n",
    "    # Step 3: Cluster to find ball center\n",
    "    if len(red_points) > 10:\n",
    "        centroid = np.mean(red_points, axis=0)\n",
    "        \n",
    "        for iteration in range(3):\n",
    "            distances = np.linalg.norm(red_points - centroid, axis=1)\n",
    "            inliers = distances < 0.06\n",
    "            if np.sum(inliers) > 10:\n",
    "                centroid = np.mean(red_points[inliers], axis=0)\n",
    "                red_points = red_points[inliers]\n",
    "        \n",
    "        print(f\"After clustering: {len(red_points)} points\")\n",
    "        \n",
    "        detected_red_pos = np.mean(red_points, axis=0)\n",
    "        detected_red_pos[2] = ball_radius\n",
    "        \n",
    "        error = np.linalg.norm(detected_red_pos - red_pos)\n",
    "        print(f\"\\nDetected: {detected_red_pos}\")\n",
    "        print(f\"Ground truth: {red_pos}\")\n",
    "        print(f\"Error: {error*1000:.1f}mm\")\n",
    "        \n",
    "        # Visualize detected ball (bright yellow)\n",
    "        meshcat.SetObject(\"perception/detected\", Sphere(0.055), Rgba(1.0, 1.0, 0.0, 0.6))\n",
    "        meshcat.SetTransform(\"perception/detected\", RigidTransform(detected_red_pos))\n",
    "        \n",
    "        # Visualize clustered red points (bright green for visibility)\n",
    "        for i in range(0, len(red_points), max(1, len(red_points)//30)):\n",
    "            meshcat.SetObject(f\"perception/red_pt_{i}\", Sphere(0.008), Rgba(0, 1, 0, 1.0))\n",
    "            meshcat.SetTransform(f\"perception/red_pt_{i}\", RigidTransform(red_points[i]))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"✓ DETECTED RED BALL - Error: {error*1000:.1f}mm\")\n",
    "        print(\"=\"*60)\n",
    "    else:\n",
    "        detected_red_pos = None\n",
    "        print(\"✗ Too few red points for clustering\")\n",
    "else:\n",
    "    detected_red_pos = None\n",
    "    print(\"✗ No elevated points found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MANIPULATION: Pick Detected Ball (Physics Simulation) ===\n",
      "\n",
      "Target ball position: [-0.00219639 -0.50580673  0.05      ]\n",
      "\n",
      "Initial gripper pose: [ 3.70783219e-04 -4.65616808e-01  6.79321579e-01]\n",
      "Grasp pose: [-0.00219639 -0.50580673  0.17      ]\n",
      "Approach pose: [-0.00219639 -0.50580673  0.27      ]\n",
      "\n",
      "Trajectory: 2.5s total\n",
      "  t=0.0s: Initial\n",
      "  t=1.0s: Approach\n",
      "  t=1.5s: Grasp pose\n",
      "  t=2.0s: Hold complete (gripper closes)\n",
      "  t=2.5s: Lift\n",
      "\n",
      "Solving IK for trajectory...\n",
      "✓ IK solved for 50/50 poses\n",
      "\n",
      "=== Building Physics Simulation ===\n",
      "Running physics simulation...\n",
      "  Gripper closes at t=1.50s\n",
      "\n",
      "============================================================\n",
      "✓ PHYSICS SIMULATION COMPLETE!\n",
      "  Total time: 3.50s\n",
      "  Ball should respond to gripper contact\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# MANIPULATION: Pick the detected red ball - WITH PHYSICS (like arc_trajectory_planning.ipynb)\n",
    "\n",
    "from pydrake.all import PiecewisePose, PiecewisePolynomial, TrajectorySource\n",
    "\n",
    "if detected_red_pos is None:\n",
    "    print(\"ERROR: No ball detected, cannot proceed with manipulation\")\n",
    "else:\n",
    "    print(\"=== MANIPULATION: Pick Detected Ball (Physics Simulation) ===\\n\")\n",
    "    \n",
    "    # Import helper from throw_helpers\n",
    "    import sys\n",
    "    sys.path.append(str(Path.cwd()))\n",
    "    from throw_helpers import create_q_knots, joint_angles_to_pose\n",
    "    \n",
    "    target_pos = detected_red_pos\n",
    "    print(f\"Target ball position: {target_pos}\\n\")\n",
    "    \n",
    "    # Get initial gripper pose (from cell 3's temp_context)\n",
    "    print(f\"Initial gripper pose: {X_WG_initial.translation()}\")\n",
    "    \n",
    "    # Ball pose (as RigidTransform)\n",
    "    X_WBall = RigidTransform(p=target_pos)\n",
    "    \n",
    "    # Design grasp pose: ABOVE the ball, gripper pointing DOWN\n",
    "    GRASP_HEIGHT = 0.12\n",
    "    APPROACH_HEIGHT = 0.1\n",
    "    GRIPPER_OPEN = 0.107\n",
    "    GRIPPER_CLOSED = 0.0\n",
    "    \n",
    "    def design_grasp_pose(X_WO: RigidTransform) -> RigidTransform:\n",
    "        \"\"\"Grasp pose above object with gripper pointing down.\"\"\"\n",
    "        return X_WO @ RigidTransform(RotationMatrix.MakeXRotation(-np.pi/2), [0, 0, GRASP_HEIGHT])\n",
    "    \n",
    "    X_WG_grasp = design_grasp_pose(X_WBall)\n",
    "    X_WG_approach = RigidTransform(p=[0, 0, APPROACH_HEIGHT]) @ X_WG_grasp\n",
    "    \n",
    "    print(f\"Grasp pose: {X_WG_grasp.translation()}\")\n",
    "    print(f\"Approach pose: {X_WG_approach.translation()}\")\n",
    "    \n",
    "    # Trajectory timing\n",
    "    t_approach = 1.0\n",
    "    t_descend = 0.5\n",
    "    t_grasp = 0.5   # Hold time for gripper to close\n",
    "    t_lift = 0.5\n",
    "    \n",
    "    t0 = 0\n",
    "    t1 = t_approach\n",
    "    t2 = t1 + t_descend\n",
    "    t3 = t2 + t_grasp\n",
    "    t4 = t3 + t_lift\n",
    "    \n",
    "    print(f\"\\nTrajectory: {t4:.1f}s total\")\n",
    "    print(f\"  t={t0:.1f}s: Initial\")\n",
    "    print(f\"  t={t1:.1f}s: Approach\")\n",
    "    print(f\"  t={t2:.1f}s: Grasp pose\")\n",
    "    print(f\"  t={t3:.1f}s: Hold complete (gripper closes)\")\n",
    "    print(f\"  t={t4:.1f}s: Lift\")\n",
    "    \n",
    "    # Create pose trajectory\n",
    "    pose_traj = PiecewisePose.MakeLinear(\n",
    "        [t0, t1, t2, t3, t4],\n",
    "        [X_WG_initial, X_WG_approach, X_WG_grasp, X_WG_grasp, X_WG_approach]\n",
    "    )\n",
    "    \n",
    "    # Gripper trajectory: open until t2, then close\n",
    "    gripper_traj = PiecewisePolynomial.FirstOrderHold(\n",
    "        [t0, t2, t2 + 0.01, t4],\n",
    "        np.array([[GRIPPER_OPEN, GRIPPER_OPEN, GRIPPER_CLOSED, GRIPPER_CLOSED]])\n",
    "    )\n",
    "    \n",
    "    # Convert poses to joint angles via IK\n",
    "    print(\"\\nSolving IK for trajectory...\")\n",
    "    num_samples = 50\n",
    "    t_samples = np.linspace(t0, t4, num_samples)\n",
    "    poses = [pose_traj.GetPose(t) for t in t_samples]\n",
    "    q_knots = create_q_knots(plant, poses)\n",
    "    \n",
    "    print(f\"✓ IK solved for {len(q_knots)}/{num_samples} poses\")\n",
    "    \n",
    "    if len(q_knots) >= 10:\n",
    "        # Create joint trajectory\n",
    "        q_traj = PiecewisePolynomial.CubicShapePreserving(\n",
    "            t_samples[:len(q_knots)], \n",
    "            np.array(q_knots).T\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n=== Building Physics Simulation ===\")\n",
    "        \n",
    "        # Rebuild diagram with trajectory sources (like arc_trajectory_planning cell 7)\n",
    "        meshcat.Delete()\n",
    "        builder2 = DiagramBuilder()\n",
    "        scenario2 = LoadScenario(data=scenario_yaml)\n",
    "        station2 = MakeHardwareStation(scenario2, meshcat=meshcat)\n",
    "        builder2.AddSystem(station2)\n",
    "        \n",
    "        # Connect trajectory sources to station inputs\n",
    "        q_source = builder2.AddSystem(TrajectorySource(q_traj))\n",
    "        g_source = builder2.AddSystem(TrajectorySource(gripper_traj))\n",
    "        \n",
    "        builder2.Connect(q_source.get_output_port(), station2.GetInputPort(\"iiwa.position\"))\n",
    "        builder2.Connect(g_source.get_output_port(), station2.GetInputPort(\"wsg.position\"))\n",
    "        \n",
    "        diagram2 = builder2.Build()\n",
    "        \n",
    "        # Run physics simulation\n",
    "        simulator2 = Simulator(diagram2)\n",
    "        simulator2.set_target_realtime_rate(1.0)\n",
    "        \n",
    "        print(f\"Running physics simulation...\")\n",
    "        print(f\"  Gripper closes at t={t2:.2f}s\")\n",
    "        \n",
    "        meshcat.StartRecording()\n",
    "        simulator2.AdvanceTo(q_traj.end_time() + 1.0)\n",
    "        meshcat.StopRecording()\n",
    "        meshcat.PublishRecording()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"✓ PHYSICS SIMULATION COMPLETE!\")\n",
    "        print(f\"  Total time: {q_traj.end_time() + 1.0:.2f}s\")\n",
    "        print(\"  Ball should respond to gripper contact\")\n",
    "        print(\"=\"*60)\n",
    "    else:\n",
    "        print(\"✗ Not enough IK solutions - check robot reachability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
