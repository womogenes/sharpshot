{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports loaded\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import time\n",
    "from pydrake.all import (\n",
    "    DiagramBuilder,\n",
    "    Simulator,\n",
    "    StartMeshcat,\n",
    "    RigidTransform,\n",
    "    RotationMatrix,\n",
    "    Sphere,\n",
    "    Box,\n",
    "    RollPitchYaw,\n",
    "    InverseKinematics,\n",
    "    Solve,\n",
    "    MakeRenderEngineVtk,\n",
    "    RenderEngineVtkParams,\n",
    ")\n",
    "from pydrake.geometry import Rgba\n",
    "from pydrake.systems.sensors import RgbdSensor, CameraConfig\n",
    "from manipulation.station import LoadScenario, MakeHardwareStation\n",
    "\n",
    "print(\"✓ Imports loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:drake:Meshcat listening for connections at http://localhost:7001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meshcat: http://localhost:7001\n"
     ]
    }
   ],
   "source": [
    "meshcat = StartMeshcat()\n",
    "print(f\"Meshcat: {meshcat.web_url()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perception functions\n",
    "\n",
    "def depth_image_to_point_cloud(depth_image, rgb_image, camera_info):\n",
    "    \"\"\"Convert depth image to colored point cloud in camera frame\"\"\"\n",
    "    height, width = depth_image.shape\n",
    "    fx = camera_info.focal_x()\n",
    "    fy = camera_info.focal_y()\n",
    "    cx = camera_info.center_x()\n",
    "    cy = camera_info.center_y()\n",
    "    \n",
    "    # Pixel grid\n",
    "    u = np.arange(width)\n",
    "    v = np.arange(height)\n",
    "    u_grid, v_grid = np.meshgrid(u, v)\n",
    "    \n",
    "    # Valid depth mask\n",
    "    valid = (depth_image > 0.01) & (depth_image < 5.0)\n",
    "    \n",
    "    u_valid = u_grid[valid]\n",
    "    v_valid = v_grid[valid]\n",
    "    z_valid = depth_image[valid]\n",
    "    \n",
    "    # Back-project to 3D (camera frame: X right, Y down, Z forward)\n",
    "    x = (u_valid - cx) * z_valid / fx\n",
    "    y = (v_valid - cy) * z_valid / fy\n",
    "    z = z_valid\n",
    "    \n",
    "    points = np.column_stack([x, y, z])\n",
    "    colors = rgb_image[valid].astype(float) / 255.0\n",
    "    \n",
    "    return points, colors\n",
    "\n",
    "\n",
    "def transform_points_to_world(points, X_CameraToWorld):\n",
    "    \"\"\"Transform points from camera frame to world frame\"\"\"\n",
    "    points_world = []\n",
    "    for pt in points:\n",
    "        pt_world = X_CameraToWorld @ pt\n",
    "        points_world.append(pt_world)\n",
    "    return np.array(points_world)\n",
    "\n",
    "\n",
    "def segment_red_color(points, colors, red_min=0.5, other_max=0.35):\n",
    "    \"\"\"Segment red points using color thresholds\"\"\"\n",
    "    is_red = (\n",
    "        (colors[:, 0] > red_min) &      # R > 0.5\n",
    "        (colors[:, 1] < other_max) &    # G < 0.35\n",
    "        (colors[:, 2] < other_max)      # B < 0.35\n",
    "    )\n",
    "    return points[is_red]\n",
    "\n",
    "\n",
    "def compute_ball_center(points):\n",
    "    \"\"\"Compute centroid of point cluster\"\"\"\n",
    "    if len(points) == 0:\n",
    "        return None\n",
    "    return np.mean(points, axis=0)\n",
    "\n",
    "\n",
    "def simulate_rgbd_capture(camera_info, X_WorldToCamera, ball_positions, ball_colors, ball_radius=0.05):\n",
    "    \"\"\"Simulate RGB-D camera seeing balls (for testing without real scene graph objects)\"\"\"\n",
    "    width = camera_info.width()\n",
    "    height = camera_info.height()\n",
    "    fx = camera_info.focal_x()\n",
    "    fy = camera_info.focal_y()\n",
    "    cx = camera_info.center_x()\n",
    "    cy = camera_info.center_y()\n",
    "    \n",
    "    # Initialize images\n",
    "    rgb_image = np.ones((height, width, 3), dtype=np.uint8) * 50  # Dark background\n",
    "    depth_image = np.zeros((height, width), dtype=np.float32)\n",
    "    \n",
    "    # Transform balls to camera frame\n",
    "    X_CameraToWorld = X_WorldToCamera.inverse()\n",
    "    \n",
    "    for ball_pos, ball_color in zip(ball_positions, ball_colors):\n",
    "        ball_in_camera = X_CameraToWorld @ ball_pos\n",
    "        \n",
    "        if ball_in_camera[2] <= 0:\n",
    "            continue\n",
    "        \n",
    "        u_center = fx * ball_in_camera[0] / ball_in_camera[2] + cx\n",
    "        v_center = fy * ball_in_camera[1] / ball_in_camera[2] + cy\n",
    "        radius_px = int(fx * ball_radius / ball_in_camera[2])\n",
    "        \n",
    "        for v in range(max(0, int(v_center - radius_px)), min(height, int(v_center + radius_px))):\n",
    "            for u in range(max(0, int(u_center - radius_px)), min(width, int(u_center + radius_px))):\n",
    "                du = u - u_center\n",
    "                dv = v - v_center\n",
    "                \n",
    "                if du**2 + dv**2 < radius_px**2:\n",
    "                    offset_from_center = np.sqrt(du**2 + dv**2) / radius_px\n",
    "                    if offset_from_center < 1.0:\n",
    "                        depth_offset = ball_radius * np.sqrt(1 - offset_from_center**2)\n",
    "                        depth = ball_in_camera[2] - depth_offset\n",
    "                        rgb_image[v, u] = ball_color\n",
    "                        depth_image[v, u] = depth\n",
    "    \n",
    "    return rgb_image, depth_image\n",
    "\n",
    "\n",
    "# Manipulation functions\n",
    "\n",
    "def compute_sphere_grasp(ball_center, ball_radius):\n",
    "    \"\"\"Compute top-down grasp pose for sphere\"\"\"\n",
    "    finger_depth = 0.01\n",
    "    grasp_height = ball_radius + finger_depth\n",
    "    p_WG = ball_center + np.array([0, 0, grasp_height])\n",
    "    R_WG = RotationMatrix(RollPitchYaw([np.pi, 0, 0]))\n",
    "    X_WG = RigidTransform(R_WG, p_WG)\n",
    "    return X_WG\n",
    "\n",
    "\n",
    "def normalize_joint_angles(q_start, q_end):\n",
    "    \"\"\"Normalize joint angles to take shortest path (prevents 360 spins!)\n",
    "    \n",
    "    For each joint, if the difference is > π, adjust by 2π to take shorter path.\n",
    "    \"\"\"\n",
    "    q_end_normalized = q_end.copy()\n",
    "    for i in range(len(q_start)):\n",
    "        diff = q_end[i] - q_start[i]\n",
    "        # If difference is > π, we're going the long way around\n",
    "        if diff > np.pi:\n",
    "            q_end_normalized[i] -= 2 * np.pi\n",
    "        elif diff < -np.pi:\n",
    "            q_end_normalized[i] += 2 * np.pi\n",
    "    return q_end_normalized\n",
    "\n",
    "\n",
    "def solve_ik_position_priority(plant, plant_context, target_position, \n",
    "                                 orientation_target=None, pos_tol=0.005):\n",
    "    \"\"\"IK solver prioritizing position over strict orientation\n",
    "    \n",
    "    Note: Bin collision geometry exists in scene graph, so the robot\n",
    "    will respect it during execution even if IK doesn't explicitly check.\n",
    "    We ensure collision-free paths by approaching from above.\n",
    "    \"\"\"\n",
    "    ik = InverseKinematics(plant, plant_context)\n",
    "    gripper_frame = plant.GetFrameByName(\"body\")\n",
    "    \n",
    "    # Position constraint\n",
    "    ik.AddPositionConstraint(\n",
    "        gripper_frame, [0, 0, 0], plant.world_frame(),\n",
    "        target_position - pos_tol, target_position + pos_tol\n",
    "    )\n",
    "    \n",
    "    # Orientation constraint\n",
    "    if orientation_target is not None:\n",
    "        if isinstance(orientation_target, RigidTransform):\n",
    "            orientation_target = orientation_target.rotation()\n",
    "        ik.AddOrientationConstraint(\n",
    "            gripper_frame, RotationMatrix(), plant.world_frame(),\n",
    "            orientation_target, 0.3\n",
    "        )\n",
    "    \n",
    "    # Cost to stay near current configuration (increased to discourage large movements)\n",
    "    q_nominal = plant.GetPositions(plant_context, plant.GetModelInstanceByName(\"iiwa\"))\n",
    "    ik.prog().AddQuadraticErrorCost(np.eye(7) * 50.0, q_nominal, ik.q())  # Increased from 5.0\n",
    "    \n",
    "    result = Solve(ik.prog())\n",
    "    if result.is_success():\n",
    "        return True, result.GetSolution(ik.q())\n",
    "    return False, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:drake:Drake materials have been assigned to a glTF file. glTF defines its own materials, so post hoc materials will be ignored and should be removed from the model specification. glTF file: '/root/.cache/drake/package_map/719c542a6323d5a8cc74fe6da84f305aebc6d7810b84cb94023d80f78e06586b-6e8a5cbbb49ceed5e2ae3a392577ca12bd01fd5983dc56151cb5fa7d55d27ed8/wsg_50_description/meshes/wsg_body.gltf'\n",
      "WARNING:drake:Drake materials have been assigned to a glTF file. glTF defines its own materials, so post hoc materials will be ignored and should be removed from the model specification. glTF file: '/root/.cache/drake/package_map/719c542a6323d5a8cc74fe6da84f305aebc6d7810b84cb94023d80f78e06586b-6e8a5cbbb49ceed5e2ae3a392577ca12bd01fd5983dc56151cb5fa7d55d27ed8/wsg_50_description/meshes/finger_with_tip.gltf'\n",
      "WARNING:drake:Drake materials have been assigned to a glTF file. glTF defines its own materials, so post hoc materials will be ignored and should be removed from the model specification. glTF file: '/root/.cache/drake/package_map/719c542a6323d5a8cc74fe6da84f305aebc6d7810b84cb94023d80f78e06586b-6e8a5cbbb49ceed5e2ae3a392577ca12bd01fd5983dc56151cb5fa7d55d27ed8/wsg_50_description/meshes/finger_with_tip.gltf'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ THROWING PIPELINE SETUP: Robot + Table + Balls + 4 CAMERAS\n",
      "  Resolution: 640x480 per camera\n",
      "  Red ball at: [ 0.   -0.5   0.05] (on table)\n",
      "  Blue ball at: [-0.15 -0.55  0.05] (on table)\n",
      "\n",
      "  Camera positions (4 cameras for full coverage):\n",
      "  Camera 1 (Cyan):   [ 0.4  -0.15  0.5 ] - Front-right, HIGH\n",
      "  Camera 2 (Orange): [-0.35 -0.9   0.25] - Back-left, LOW\n",
      "  Camera 3 (Purple): [-0.5 -0.4  0.4] - Left side, MEDIUM\n",
      "  Camera 4 (Teal):   [ 0.5  -0.6   0.35] - RIGHT SIDE (NEW!)\n",
      "\n",
      "✓ Full 360° coverage around workspace!\n"
     ]
    }
   ],
   "source": [
    "# Build scene with robot, gripper, TABLE, and 4 CAMERAS (throwing pipeline setup)\n",
    "\n",
    "import tempfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create temporary directory for SDF files\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "# Red ball SDF\n",
    "red_ball_sdf = \"\"\"<?xml version=\"1.0\"?>\n",
    "<sdf version=\"1.7\">\n",
    "  <model name=\"red_ball\">\n",
    "    <link name=\"ball\">\n",
    "      <inertial>\n",
    "        <mass>0.1</mass>\n",
    "        <inertia>\n",
    "          <ixx>0.0001</ixx>\n",
    "          <ixy>0</ixy>\n",
    "          <ixz>0</ixz>\n",
    "          <iyy>0.0001</iyy>\n",
    "          <iyz>0</iyz>\n",
    "          <izz>0.0001</izz>\n",
    "        </inertia>\n",
    "      </inertial>\n",
    "      <visual name=\"visual\">\n",
    "        <geometry>\n",
    "          <sphere>\n",
    "            <radius>0.05</radius>\n",
    "          </sphere>\n",
    "        </geometry>\n",
    "        <material>\n",
    "          <diffuse>0.9 0.1 0.1 1.0</diffuse>\n",
    "        </material>\n",
    "      </visual>\n",
    "      <collision name=\"collision\">\n",
    "        <geometry>\n",
    "          <sphere>\n",
    "            <radius>0.05</radius>\n",
    "          </sphere>\n",
    "        </geometry>\n",
    "      </collision>\n",
    "    </link>\n",
    "  </model>\n",
    "</sdf>\n",
    "\"\"\"\n",
    "\n",
    "# Blue ball SDF\n",
    "blue_ball_sdf = \"\"\"<?xml version=\"1.0\"?>\n",
    "<sdf version=\"1.7\">\n",
    "  <model name=\"blue_ball\">\n",
    "    <link name=\"ball\">\n",
    "      <inertial>\n",
    "        <mass>0.1</mass>\n",
    "        <inertia>\n",
    "          <ixx>0.0001</ixx>\n",
    "          <ixy>0</ixy>\n",
    "          <ixz>0</ixz>\n",
    "          <iyy>0.0001</iyy>\n",
    "          <iyz>0</iyz>\n",
    "          <izz>0.0001</izz>\n",
    "        </inertia>\n",
    "      </inertial>\n",
    "      <visual name=\"visual\">\n",
    "        <geometry>\n",
    "          <sphere>\n",
    "            <radius>0.05</radius>\n",
    "          </sphere>\n",
    "        </geometry>\n",
    "        <material>\n",
    "          <diffuse>0.2 0.4 0.9 1.0</diffuse>\n",
    "        </material>\n",
    "      </visual>\n",
    "      <collision name=\"collision\">\n",
    "        <geometry>\n",
    "          <sphere>\n",
    "            <radius>0.05</radius>\n",
    "          </sphere>\n",
    "        </geometry>\n",
    "      </collision>\n",
    "    </link>\n",
    "  </model>\n",
    "</sdf>\n",
    "\"\"\"\n",
    "\n",
    "# Write SDF files\n",
    "red_ball_path = os.path.join(temp_dir, \"red_ball.sdf\")\n",
    "blue_ball_path = os.path.join(temp_dir, \"blue_ball.sdf\")\n",
    "\n",
    "with open(red_ball_path, 'w') as f:\n",
    "    f.write(red_ball_sdf)\n",
    "    \n",
    "with open(blue_ball_path, 'w') as f:\n",
    "    f.write(blue_ball_sdf)\n",
    "\n",
    "# Ball positions - on TABLE surface (table top at z=0, ball radius=0.05)\n",
    "ball_radius = 0.05\n",
    "red_pos = np.array([0.0, -0.5, ball_radius])   # On table, in front of robot\n",
    "blue_pos = np.array([-0.15, -0.55, ball_radius])  # Slightly to the left\n",
    "\n",
    "# Table SDF path (from assets)\n",
    "abs_table_sdf_path = f\"{Path.cwd()}/assets/table.sdf\"\n",
    "\n",
    "# Robot and gripper scenario WITH TABLE (throwing pipeline setup)\n",
    "scenario_yaml = f\"\"\"\n",
    "directives:\n",
    "- add_model:\n",
    "    name: iiwa\n",
    "    file: package://drake_models/iiwa_description/sdf/iiwa7_no_collision.sdf\n",
    "    default_joint_positions:\n",
    "      iiwa_joint_1: [-1.57]\n",
    "      iiwa_joint_2: [0.1]\n",
    "      iiwa_joint_3: [0]\n",
    "      iiwa_joint_4: [-1.2]\n",
    "      iiwa_joint_5: [0]\n",
    "      iiwa_joint_6: [1.6]\n",
    "      iiwa_joint_7: [0]\n",
    "- add_weld:\n",
    "    parent: world\n",
    "    child: iiwa::iiwa_link_0\n",
    "- add_model:\n",
    "    name: wsg\n",
    "    file: package://manipulation/hydro/schunk_wsg_50_with_tip.sdf\n",
    "- add_weld:\n",
    "    parent: iiwa::iiwa_link_7\n",
    "    child: wsg::body\n",
    "    X_PC:\n",
    "      translation: [0, 0, 0.09]\n",
    "      rotation: !Rpy {{ deg: [90, 0, 90] }}\n",
    "- add_model:\n",
    "    name: table\n",
    "    file: file://{abs_table_sdf_path}\n",
    "- add_weld:\n",
    "    parent: world\n",
    "    child: table::table_link\n",
    "    X_PC:\n",
    "      translation: [0.0, 0.0, -0.05]\n",
    "      rotation: !Rpy {{ deg: [0, 0, -90] }}\n",
    "- add_model:\n",
    "    name: red_ball\n",
    "    file: file://{red_ball_path}\n",
    "- add_weld:\n",
    "    parent: world\n",
    "    child: red_ball::ball\n",
    "    X_PC:\n",
    "      translation: [{red_pos[0]}, {red_pos[1]}, {red_pos[2]}]\n",
    "- add_model:\n",
    "    name: blue_ball\n",
    "    file: file://{blue_ball_path}\n",
    "- add_weld:\n",
    "    parent: world\n",
    "    child: blue_ball::ball\n",
    "    X_PC:\n",
    "      translation: [{blue_pos[0]}, {blue_pos[1]}, {blue_pos[2]}]\n",
    "\"\"\"\n",
    "\n",
    "scenario = LoadScenario(data=scenario_yaml)\n",
    "builder = DiagramBuilder()\n",
    "\n",
    "# Add hardware station\n",
    "station = MakeHardwareStation(scenario, meshcat=meshcat)\n",
    "builder.AddSystem(station)\n",
    "\n",
    "# Get scene graph and add a renderer for the camera\n",
    "scene_graph = station.GetSubsystemByName(\"scene_graph\")\n",
    "\n",
    "# Add VTK renderer to scene graph\n",
    "renderer_name = \"my_renderer\"\n",
    "params = RenderEngineVtkParams()\n",
    "renderer = MakeRenderEngineVtk(params)\n",
    "scene_graph.AddRenderer(renderer_name, renderer)\n",
    "\n",
    "# Camera setup using CameraConfig\n",
    "width, height = 640, 480\n",
    "\n",
    "camera_config = CameraConfig()\n",
    "camera_config.width = width\n",
    "camera_config.height = height\n",
    "camera_config.fps = 10.0\n",
    "camera_config.renderer_name = renderer_name\n",
    "\n",
    "# Helper function to create camera transform\n",
    "def make_camera_transform(camera_pos, look_at_pos):\n",
    "    \"\"\"Create camera transform that looks from camera_pos to look_at_pos\"\"\"\n",
    "    forward = look_at_pos - camera_pos\n",
    "    forward = forward / np.linalg.norm(forward)\n",
    "    world_up = np.array([0, 0, 1])\n",
    "    right = np.cross(forward, world_up)\n",
    "    right = right / np.linalg.norm(right)\n",
    "    down = np.cross(forward, right)\n",
    "    R_WorldToCamera = RotationMatrix(np.column_stack([right, down, forward]))\n",
    "    return RigidTransform(R_WorldToCamera, camera_pos)\n",
    "\n",
    "# Define 4 camera positions SPREAD AROUND the workspace\n",
    "workspace_center = np.array([0.0, -0.5, 0.05])\n",
    "\n",
    "# Camera 1: Front-right, high angle\n",
    "camera_1_pos = np.array([0.4, -0.15, 0.5])\n",
    "\n",
    "# Camera 2: Back-left, low angle\n",
    "camera_2_pos = np.array([-0.35, -0.9, 0.25])\n",
    "\n",
    "# Camera 3: Left side, medium height\n",
    "camera_3_pos = np.array([-0.5, -0.4, 0.4])\n",
    "\n",
    "# Camera 4: RIGHT SIDE - filling the gap!\n",
    "camera_4_pos = np.array([0.5, -0.6, 0.35])\n",
    "\n",
    "X_WorldToCamera_1 = make_camera_transform(camera_1_pos, workspace_center)\n",
    "X_WorldToCamera_2 = make_camera_transform(camera_2_pos, workspace_center)\n",
    "X_WorldToCamera_3 = make_camera_transform(camera_3_pos, workspace_center)\n",
    "X_WorldToCamera_4 = make_camera_transform(camera_4_pos, workspace_center)\n",
    "\n",
    "# Create 4 sets of cameras\n",
    "color_camera_1, depth_camera_1 = camera_config.MakeCameras()\n",
    "color_camera_2, depth_camera_2 = camera_config.MakeCameras()\n",
    "color_camera_3, depth_camera_3 = camera_config.MakeCameras()\n",
    "color_camera_4, depth_camera_4 = camera_config.MakeCameras()\n",
    "\n",
    "# Add 4 RgbdSensors to diagram\n",
    "rgbd_sensor_1 = builder.AddSystem(RgbdSensor(\n",
    "    parent_id=scene_graph.world_frame_id(),\n",
    "    X_PB=X_WorldToCamera_1,\n",
    "    color_camera=color_camera_1,\n",
    "    depth_camera=depth_camera_1\n",
    "))\n",
    "\n",
    "rgbd_sensor_2 = builder.AddSystem(RgbdSensor(\n",
    "    parent_id=scene_graph.world_frame_id(),\n",
    "    X_PB=X_WorldToCamera_2,\n",
    "    color_camera=color_camera_2,\n",
    "    depth_camera=depth_camera_2\n",
    "))\n",
    "\n",
    "rgbd_sensor_3 = builder.AddSystem(RgbdSensor(\n",
    "    parent_id=scene_graph.world_frame_id(),\n",
    "    X_PB=X_WorldToCamera_3,\n",
    "    color_camera=color_camera_3,\n",
    "    depth_camera=depth_camera_3\n",
    "))\n",
    "\n",
    "rgbd_sensor_4 = builder.AddSystem(RgbdSensor(\n",
    "    parent_id=scene_graph.world_frame_id(),\n",
    "    X_PB=X_WorldToCamera_4,\n",
    "    color_camera=color_camera_4,\n",
    "    depth_camera=depth_camera_4\n",
    "))\n",
    "\n",
    "# Connect all 4 sensors to scene graph\n",
    "builder.Connect(station.GetOutputPort(\"query_object\"), rgbd_sensor_1.query_object_input_port())\n",
    "builder.Connect(station.GetOutputPort(\"query_object\"), rgbd_sensor_2.query_object_input_port())\n",
    "builder.Connect(station.GetOutputPort(\"query_object\"), rgbd_sensor_3.query_object_input_port())\n",
    "builder.Connect(station.GetOutputPort(\"query_object\"), rgbd_sensor_4.query_object_input_port())\n",
    "\n",
    "# Build complete diagram\n",
    "diagram = builder.Build()\n",
    "\n",
    "# Initialize simulation\n",
    "simulator = Simulator(diagram)\n",
    "simulator.set_target_realtime_rate(1.0)\n",
    "context = simulator.get_mutable_context()\n",
    "\n",
    "plant = station.GetSubsystemByName(\"plant\")\n",
    "plant_context = plant.GetMyContextFromRoot(context)\n",
    "\n",
    "# Home position (same as base_throw_environ.py)\n",
    "q_home = np.array([-1.57, 0.1, 0, -1.2, 0, 1.6, 0])\n",
    "plant.SetPositions(plant_context, plant.GetModelInstanceByName(\"iiwa\"), q_home)\n",
    "\n",
    "simulator.AdvanceTo(0.1)\n",
    "\n",
    "# Visualize all 4 cameras in meshcat\n",
    "from pydrake.geometry import Cylinder\n",
    "\n",
    "camera_colors = [\n",
    "    Rgba(0.0, 0.5, 1.0, 0.8),   # Cyan - Camera 1\n",
    "    Rgba(1.0, 0.5, 0.0, 0.8),   # Orange - Camera 2\n",
    "    Rgba(0.8, 0.0, 0.8, 0.8),   # Purple - Camera 3\n",
    "    Rgba(0.0, 1.0, 0.5, 0.8),   # Teal/Green - Camera 4 (NEW!)\n",
    "]\n",
    "\n",
    "camera_transforms = [X_WorldToCamera_1, X_WorldToCamera_2, X_WorldToCamera_3, X_WorldToCamera_4]\n",
    "camera_positions = [camera_1_pos, camera_2_pos, camera_3_pos, camera_4_pos]\n",
    "\n",
    "for i, (X_cam, pos, color) in enumerate(zip(camera_transforms, camera_positions, camera_colors), 1):\n",
    "    meshcat.SetObject(f\"camera{i}/body\", Box(0.05, 0.05, 0.08), color)\n",
    "    meshcat.SetTransform(f\"camera{i}/body\", X_cam)\n",
    "    \n",
    "    lens_color = Rgba(color.r() * 0.5, color.g() * 0.5, color.b() * 0.5, 0.9)\n",
    "    meshcat.SetObject(f\"camera{i}/lens\", Cylinder(0.02, 0.03), lens_color)\n",
    "    meshcat.SetTransform(f\"camera{i}/lens\", X_cam @ RigidTransform([0, 0, 0.05]))\n",
    "\n",
    "# Save camera info and sensors for perception\n",
    "camera_info_1 = color_camera_1.core().intrinsics()\n",
    "camera_info_2 = color_camera_2.core().intrinsics()\n",
    "camera_info_3 = color_camera_3.core().intrinsics()\n",
    "camera_info_4 = color_camera_4.core().intrinsics()\n",
    "\n",
    "# Store for later use - NOW 4 CAMERAS\n",
    "camera_sensors = [rgbd_sensor_1, rgbd_sensor_2, rgbd_sensor_3, rgbd_sensor_4]\n",
    "camera_infos = [camera_info_1, camera_info_2, camera_info_3, camera_info_4]\n",
    "camera_transforms = [X_WorldToCamera_1, X_WorldToCamera_2, X_WorldToCamera_3, X_WorldToCamera_4]\n",
    "\n",
    "# Save ball positions for later\n",
    "blue_positions = [blue_pos]\n",
    "\n",
    "print(\"✓ THROWING PIPELINE SETUP: Robot + Table + Balls + 4 CAMERAS\")\n",
    "print(f\"  Resolution: {width}x{height} per camera\")\n",
    "print(f\"  Red ball at: {red_pos} (on table)\")\n",
    "print(f\"  Blue ball at: {blue_pos} (on table)\")\n",
    "print(f\"\\n  Camera positions (4 cameras for full coverage):\")\n",
    "print(f\"  Camera 1 (Cyan):   {camera_1_pos} - Front-right, HIGH\")\n",
    "print(f\"  Camera 2 (Orange): {camera_2_pos} - Back-left, LOW\")\n",
    "print(f\"  Camera 3 (Purple): {camera_3_pos} - Left side, MEDIUM\")\n",
    "print(f\"  Camera 4 (Teal):   {camera_4_pos} - RIGHT SIDE (NEW!)\")\n",
    "print(f\"\\n✓ Full 360° coverage around workspace!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Scene ready for perception\n",
      "  Red ball ground truth: [ 0.   -0.5   0.05]\n",
      "  Blue ball ground truth: [-0.15 -0.55  0.05]\n"
     ]
    }
   ],
   "source": [
    "# Visualization helpers (no bin in throwing pipeline)\n",
    "\n",
    "# Balls are already in scene graph from cell 3\n",
    "# Just add visual markers for the ball positions\n",
    "meshcat.SetObject(\"markers/red_target\", Sphere(0.01), Rgba(1, 0.5, 0, 0.8))\n",
    "meshcat.SetTransform(\"markers/red_target\", RigidTransform(red_pos))\n",
    "\n",
    "meshcat.SetObject(\"markers/blue_target\", Sphere(0.01), Rgba(0, 0.5, 1, 0.8))\n",
    "meshcat.SetTransform(\"markers/blue_target\", RigidTransform(blue_pos))\n",
    "\n",
    "print(\"✓ Scene ready for perception\")\n",
    "print(f\"  Red ball ground truth: {red_pos}\")\n",
    "print(f\"  Blue ball ground truth: {blue_pos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MULTI-CAMERA PERCEPTION PIPELINE ===\n",
      "\n",
      "--- Camera 1 ---\n",
      "  Generated 266226 points\n",
      "--- Camera 2 ---\n",
      "  Generated 246299 points\n",
      "--- Camera 3 ---\n",
      "  Generated 270377 points\n",
      "--- Camera 4 ---\n",
      "  Generated 270942 points\n",
      "\n",
      "✓ Merged 1053844 points from 4 cameras\n",
      "\n",
      "--- Visualizing Point Clouds (color-coded by camera) ---\n",
      "  Camera 1: plotting 887 points (Cyan)\n",
      "  Camera 2: plotting 820 points (Orange)\n",
      "  Camera 3: plotting 901 points (Purple)\n",
      "  Camera 4: plotting 903 points (Teal)\n",
      "  ✓ Plotted ~3512 points total\n",
      "  Legend: Cyan=Cam1, Orange=Cam2, Purple=Cam3, Teal=Cam4\n",
      "\n",
      "--- Ball Detection ---\n",
      "Points above table (z > 0.02): 83670\n",
      "Reddest 4183 elevated points selected\n",
      "After clustering: 4244 points\n",
      "\n",
      "Detected: [-0.00219639 -0.50580673  0.05      ]\n",
      "Ground truth: [ 0.   -0.5   0.05]\n",
      "Error: 6.2mm\n",
      "\n",
      "============================================================\n",
      "✓ DETECTED RED BALL - Error: 6.2mm\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# PERCEPTION: Multi-camera capture and point cloud fusion (4 CAMERAS)\n",
    "\n",
    "print(\"=== MULTI-CAMERA PERCEPTION PIPELINE ===\\n\")\n",
    "\n",
    "# Lists to store point clouds from all cameras\n",
    "all_points_world = []\n",
    "all_colors = []\n",
    "\n",
    "# Process each camera\n",
    "for cam_idx, (sensor, cam_info, X_cam) in enumerate(zip(camera_sensors, camera_infos, camera_transforms), 1):\n",
    "    print(f\"--- Camera {cam_idx} ---\")\n",
    "    \n",
    "    # Get images from this sensor\n",
    "    sensor_context = sensor.GetMyContextFromRoot(context)\n",
    "    \n",
    "    color_image = sensor.color_image_output_port().Eval(sensor_context)\n",
    "    rgb_img = color_image.data[:, :, :3].astype(np.uint8)\n",
    "    \n",
    "    depth_image = sensor.depth_image_32F_output_port().Eval(sensor_context)\n",
    "    depth_img = depth_image.data.squeeze().astype(np.float32)\n",
    "    \n",
    "    # Convert to point cloud in camera frame\n",
    "    points_camera, colors = depth_image_to_point_cloud(depth_img, rgb_img, cam_info)\n",
    "    print(f\"  Generated {len(points_camera)} points\")\n",
    "    \n",
    "    # Transform to world frame\n",
    "    points_world = transform_points_to_world(points_camera, X_cam)\n",
    "    \n",
    "    all_points_world.append(points_world)\n",
    "    all_colors.append(colors)\n",
    "\n",
    "# Merge all point clouds\n",
    "merged_points = np.vstack(all_points_world)\n",
    "merged_colors = np.vstack(all_colors)\n",
    "\n",
    "print(f\"\\n✓ Merged {len(merged_points)} points from {len(camera_sensors)} cameras\\n\")\n",
    "\n",
    "# VISUALIZE point clouds from each camera with DIFFERENT COLORS\n",
    "print(\"--- Visualizing Point Clouds (color-coded by camera) ---\")\n",
    "cam_viz_colors = [\n",
    "    Rgba(0.0, 0.5, 1.0, 0.9),   # Cyan - Camera 1\n",
    "    Rgba(1.0, 0.5, 0.0, 0.9),   # Orange - Camera 2  \n",
    "    Rgba(0.8, 0.0, 0.8, 0.9),   # Purple - Camera 3\n",
    "    Rgba(0.0, 1.0, 0.5, 0.9),   # Teal - Camera 4\n",
    "]\n",
    "cam_names = ['Cyan', 'Orange', 'Purple', 'Teal']\n",
    "\n",
    "# Downsample for visualization\n",
    "downsample = 300\n",
    "\n",
    "for cam_idx, (points, viz_color) in enumerate(zip(all_points_world, cam_viz_colors)):\n",
    "    print(f\"  Camera {cam_idx+1}: plotting {len(points)//downsample} points ({cam_names[cam_idx]})\")\n",
    "    for i in range(0, len(points), downsample):\n",
    "        meshcat.SetObject(f\"pointcloud/cam{cam_idx+1}/pt_{i}\", Sphere(0.004), viz_color)\n",
    "        meshcat.SetTransform(f\"pointcloud/cam{cam_idx+1}/pt_{i}\", RigidTransform(points[i]))\n",
    "\n",
    "print(f\"  ✓ Plotted ~{len(merged_points)//downsample} points total\")\n",
    "print(f\"  Legend: Cyan=Cam1, Orange=Cam2, Purple=Cam3, Teal=Cam4\\n\")\n",
    "\n",
    "# BALL DETECTION\n",
    "print(\"--- Ball Detection ---\")\n",
    "\n",
    "# Step 1: Find points that are ABOVE the table (z > 0.02)\n",
    "above_table_mask = merged_points[:, 2] > 0.02\n",
    "above_table_points = merged_points[above_table_mask]\n",
    "above_table_colors = merged_colors[above_table_mask]\n",
    "print(f\"Points above table (z > 0.02): {len(above_table_points)}\")\n",
    "\n",
    "# Step 2: Among elevated points, find the reddest ones\n",
    "if len(above_table_points) > 0:\n",
    "    redness = above_table_colors[:, 0] - np.maximum(above_table_colors[:, 1], above_table_colors[:, 2])\n",
    "    \n",
    "    num_red_points = max(100, int(len(above_table_points) * 0.05))\n",
    "    red_threshold = np.sort(redness)[-num_red_points]\n",
    "    \n",
    "    is_red = redness >= red_threshold\n",
    "    red_points = above_table_points[is_red]\n",
    "    print(f\"Reddest {num_red_points} elevated points selected\")\n",
    "    \n",
    "    # Step 3: Cluster to find ball center\n",
    "    if len(red_points) > 10:\n",
    "        centroid = np.mean(red_points, axis=0)\n",
    "        \n",
    "        for iteration in range(3):\n",
    "            distances = np.linalg.norm(red_points - centroid, axis=1)\n",
    "            inliers = distances < 0.06\n",
    "            if np.sum(inliers) > 10:\n",
    "                centroid = np.mean(red_points[inliers], axis=0)\n",
    "                red_points = red_points[inliers]\n",
    "        \n",
    "        print(f\"After clustering: {len(red_points)} points\")\n",
    "        \n",
    "        detected_red_pos = np.mean(red_points, axis=0)\n",
    "        detected_red_pos[2] = ball_radius\n",
    "        \n",
    "        error = np.linalg.norm(detected_red_pos - red_pos)\n",
    "        print(f\"\\nDetected: {detected_red_pos}\")\n",
    "        print(f\"Ground truth: {red_pos}\")\n",
    "        print(f\"Error: {error*1000:.1f}mm\")\n",
    "        \n",
    "        # Visualize detected ball (bright yellow)\n",
    "        meshcat.SetObject(\"perception/detected\", Sphere(0.055), Rgba(1.0, 1.0, 0.0, 0.6))\n",
    "        meshcat.SetTransform(\"perception/detected\", RigidTransform(detected_red_pos))\n",
    "        \n",
    "        # Visualize clustered red points (bright green for visibility)\n",
    "        for i in range(0, len(red_points), max(1, len(red_points)//30)):\n",
    "            meshcat.SetObject(f\"perception/red_pt_{i}\", Sphere(0.008), Rgba(0, 1, 0, 1.0))\n",
    "            meshcat.SetTransform(f\"perception/red_pt_{i}\", RigidTransform(red_points[i]))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"✓ DETECTED RED BALL - Error: {error*1000:.1f}mm\")\n",
    "        print(\"=\"*60)\n",
    "    else:\n",
    "        detected_red_pos = None\n",
    "        print(\"✗ Too few red points for clustering\")\n",
    "else:\n",
    "    detected_red_pos = None\n",
    "    print(\"✗ No elevated points found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MANIPULATION: Pick Detected Ball ===\n",
      "\n",
      "Target ball position: [-0.00219639 -0.50580673  0.05      ]\n",
      "\n",
      "WSG gripper has 2 position(s)\n",
      "iiwa robot has 7 joints\n",
      "\n",
      "Step 0: Moving to HOME position...\n",
      "  ✓ At home, gripper open\n",
      "\n",
      "Planning waypoints:\n",
      "  ✓ Pre-grasp at [-0.00219639 -0.50580673  0.2       ]\n",
      "  ✗ Grasp at [-0.00219639 -0.50580673  0.11      ]\n",
      "  ✗ Lift at [-0.00219639 -0.50580673  0.2       ]\n",
      "\n",
      "✓ Planned 1/3 waypoints\n",
      "\n",
      "✗ Too few waypoints succeeded\n"
     ]
    }
   ],
   "source": [
    "# MANIPULATION: Pick the detected red ball WITH GRIPPER CONTROL\n",
    "\n",
    "if detected_red_pos is None:\n",
    "    print(\"ERROR: No ball detected, cannot proceed with manipulation\")\n",
    "else:\n",
    "    print(\"=== MANIPULATION: Pick Detected Ball ===\\n\")\n",
    "    \n",
    "    # Use perception-detected position\n",
    "    target_pos = detected_red_pos\n",
    "    print(f\"Target ball position: {target_pos}\\n\")\n",
    "    \n",
    "    # Get WSG gripper info\n",
    "    wsg = plant.GetModelInstanceByName(\"wsg\")\n",
    "    num_wsg_positions = plant.num_positions(wsg)\n",
    "    print(f\"WSG gripper has {num_wsg_positions} position(s)\")\n",
    "    \n",
    "    # Get iiwa info\n",
    "    iiwa = plant.GetModelInstanceByName(\"iiwa\")\n",
    "    num_iiwa_positions = plant.num_positions(iiwa)\n",
    "    print(f\"iiwa robot has {num_iiwa_positions} joints\")\n",
    "    \n",
    "    # Move to home first with gripper OPEN\n",
    "    print(\"\\nStep 0: Moving to HOME position...\")\n",
    "    plant.SetPositions(plant_context, iiwa, q_home)\n",
    "    \n",
    "    # Open gripper\n",
    "    if num_wsg_positions == 2:\n",
    "        plant.SetPositions(plant_context, wsg, [-0.05, 0.05])\n",
    "    elif num_wsg_positions > 0:\n",
    "        plant.SetPositions(plant_context, wsg, [0.1])  # Single position = opening width\n",
    "    \n",
    "    diagram.ForcedPublish(context)\n",
    "    time.sleep(0.3)\n",
    "    print(\"  ✓ At home, gripper open\\n\")\n",
    "    \n",
    "    # Compute grasp pose\n",
    "    X_WG_grasp = compute_sphere_grasp(target_pos, ball_radius)\n",
    "    \n",
    "    # IK solver that works with iiwa7\n",
    "    def solve_ik_for_iiwa7(plant, plant_context, target_position, orientation_target=None, pos_tol=0.005):\n",
    "        \"\"\"IK solver for iiwa7\"\"\"\n",
    "        ik = InverseKinematics(plant, plant_context)\n",
    "        gripper_frame = plant.GetFrameByName(\"body\")\n",
    "        \n",
    "        # Position constraint\n",
    "        ik.AddPositionConstraint(\n",
    "            gripper_frame, [0, 0, 0], plant.world_frame(),\n",
    "            target_position - pos_tol, target_position + pos_tol\n",
    "        )\n",
    "        \n",
    "        # Orientation constraint (relaxed)\n",
    "        if orientation_target is not None:\n",
    "            if isinstance(orientation_target, RigidTransform):\n",
    "                orientation_target = orientation_target.rotation()\n",
    "            ik.AddOrientationConstraint(\n",
    "                gripper_frame, RotationMatrix(), plant.world_frame(),\n",
    "                orientation_target, 0.3\n",
    "            )\n",
    "        \n",
    "        # Get current iiwa positions as nominal\n",
    "        q_nominal = plant.GetPositions(plant_context, iiwa)\n",
    "        \n",
    "        # Add cost to stay near current config\n",
    "        ik.prog().AddQuadraticErrorCost(\n",
    "            np.eye(num_iiwa_positions) * 10.0, \n",
    "            q_nominal, \n",
    "            ik.q()[:num_iiwa_positions]\n",
    "        )\n",
    "        \n",
    "        result = Solve(ik.prog())\n",
    "        if result.is_success():\n",
    "            return True, result.GetSolution(ik.q())[:num_iiwa_positions]\n",
    "        return False, None\n",
    "    \n",
    "    # Plan waypoints\n",
    "    print(\"Planning waypoints:\")\n",
    "    \n",
    "    # Waypoint 1: Pre-grasp (15cm above ball)\n",
    "    pre_grasp_pos = target_pos + np.array([0, 0, 0.15])\n",
    "    success_pre, q_pregrasp = solve_ik_for_iiwa7(\n",
    "        plant, plant_context, pre_grasp_pos, \n",
    "        orientation_target=X_WG_grasp.rotation(), pos_tol=0.01\n",
    "    )\n",
    "    \n",
    "    # Waypoint 2: Grasp (at ball)\n",
    "    grasp_pos = X_WG_grasp.translation()\n",
    "    success_grasp, q_grasp = solve_ik_for_iiwa7(\n",
    "        plant, plant_context, grasp_pos, \n",
    "        orientation_target=X_WG_grasp.rotation(), pos_tol=0.01\n",
    "    )\n",
    "    \n",
    "    # Waypoint 3: Lift (15cm above ball)\n",
    "    lift_pos = target_pos + np.array([0, 0, 0.15])\n",
    "    success_lift, q_lift = solve_ik_for_iiwa7(\n",
    "        plant, plant_context, lift_pos,\n",
    "        orientation_target=None,\n",
    "        pos_tol=0.02\n",
    "    )\n",
    "    \n",
    "    # Build waypoint list\n",
    "    waypoints = []\n",
    "    waypoint_names = []\n",
    "    successes = [success_pre, success_grasp, success_lift]\n",
    "    names = [\"Pre-grasp\", \"Grasp\", \"Lift\"]\n",
    "    qs = [q_pregrasp, q_grasp, q_lift]\n",
    "    positions = [pre_grasp_pos, grasp_pos, lift_pos]\n",
    "    \n",
    "    for success, name, q, pos in zip(successes, names, qs, positions):\n",
    "        status = \"✓\" if success else \"✗\"\n",
    "        print(f\"  {status} {name} at {pos}\")\n",
    "        if success:\n",
    "            waypoints.append(q)\n",
    "            waypoint_names.append(name)\n",
    "    \n",
    "    print(f\"\\n✓ Planned {len(waypoints)}/{len(names)} waypoints\\n\")\n",
    "    \n",
    "    # Visualize waypoints\n",
    "    if success_pre:\n",
    "        meshcat.SetObject(\"waypoints/pregrasp\", Sphere(0.02), Rgba(1, 1, 0, 0.6))\n",
    "        meshcat.SetTransform(\"waypoints/pregrasp\", RigidTransform(pre_grasp_pos))\n",
    "    if success_grasp:\n",
    "        meshcat.SetObject(\"waypoints/grasp\", Sphere(0.02), Rgba(0, 1, 0, 0.8))\n",
    "        meshcat.SetTransform(\"waypoints/grasp\", RigidTransform(grasp_pos))\n",
    "    if success_lift:\n",
    "        meshcat.SetObject(\"waypoints/lift\", Sphere(0.02), Rgba(0, 1, 1, 0.6))\n",
    "        meshcat.SetTransform(\"waypoints/lift\", RigidTransform(lift_pos))\n",
    "    \n",
    "    # Execute motion with gripper control\n",
    "    if len(waypoints) >= 2:\n",
    "        print(\"=== Executing motion sequence WITH GRIPPER ===\\n\")\n",
    "        \n",
    "        meshcat.StartRecording()\n",
    "        \n",
    "        all_waypoints = [q_home] + waypoints\n",
    "        all_names = [\"Home\"] + waypoint_names\n",
    "        \n",
    "        sim_time = 0.0\n",
    "        dt = 0.03\n",
    "        \n",
    "        for i in range(len(all_waypoints) - 1):\n",
    "            q_start = all_waypoints[i]\n",
    "            q_end = normalize_joint_angles(q_start, all_waypoints[i + 1])\n",
    "            name = all_names[i + 1]\n",
    "            \n",
    "            # Determine gripper state\n",
    "            gripper_open = (name != \"Grasp\")\n",
    "            \n",
    "            print(f\"  → Moving to {name}...\")\n",
    "            \n",
    "            num_steps = 35\n",
    "            for step in range(num_steps + 1):\n",
    "                alpha = step / num_steps\n",
    "                q_interp = (1 - alpha) * q_start + alpha * q_end\n",
    "                \n",
    "                # Set robot position\n",
    "                plant.SetPositions(plant_context, iiwa, q_interp)\n",
    "                \n",
    "                # Set gripper position\n",
    "                if num_wsg_positions == 2:\n",
    "                    if gripper_open:\n",
    "                        plant.SetPositions(plant_context, wsg, [-0.05, 0.05])\n",
    "                    else:\n",
    "                        plant.SetPositions(plant_context, wsg, [-0.01, 0.01])\n",
    "                elif num_wsg_positions > 0:\n",
    "                    if gripper_open:\n",
    "                        plant.SetPositions(plant_context, wsg, [0.1])\n",
    "                    else:\n",
    "                        plant.SetPositions(plant_context, wsg, [0.0])\n",
    "                \n",
    "                context.SetTime(sim_time)\n",
    "                diagram.ForcedPublish(context)\n",
    "                \n",
    "                time.sleep(dt)\n",
    "                sim_time += dt\n",
    "            \n",
    "            grip_status = \"OPEN\" if gripper_open else \"CLOSED\"\n",
    "            print(f\"    ✓ {name} (gripper: {grip_status})\")\n",
    "            time.sleep(0.3)\n",
    "            sim_time += 0.3\n",
    "        \n",
    "        meshcat.StopRecording()\n",
    "        meshcat.PublishRecording()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"SUCCESS! PERCEPTION + MANIPULATION COMPLETE\")\n",
    "        print(\"=\"*60)\n",
    "    else:\n",
    "        print(\"✗ Too few waypoints succeeded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
